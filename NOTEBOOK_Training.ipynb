{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainSetBuilder import *\n",
    "from CamemBERT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "molecular-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data array...\n",
      "...Done\n",
      "Data tokenization...\n",
      "... Done\n",
      "Tensors encoding...\n",
      "... Done\n",
      "Data handler encoding...\n",
      "... Done\n",
      "End of dataset encoding.\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset instance\n",
    "dataset = TrainSetBuilder()\n",
    "# Import allo_cine data\n",
    "dataset.import_allocine_data(reduce=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appointed-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instanciate a model \n",
    "torch.cuda.empty_cache()\n",
    "model = CamemBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0 - Total_idx 0 - Train Loss: 0.6996521949768066 - Test Loss: 0.689316314458847\n",
      "Epoch 0, step 50 - Total_idx 1 - Train Loss: 0.6927524173259735 - Test Loss: 0.6997761428356171\n",
      "Epoch 0, step 100 - Total_idx 2 - Train Loss: 0.6948449540138245 - Test Loss: 0.6959283709526062\n",
      "Epoch 0, step 150 - Total_idx 3 - Train Loss: 0.6870484590530396 - Test Loss: 0.6858777940273285\n",
      "Epoch 0, step 200 - Total_idx 4 - Train Loss: 0.6869451344013214 - Test Loss: 0.6928420126438141\n",
      "Epoch 0, step 250 - Total_idx 5 - Train Loss: 0.6830643832683563 - Test Loss: 0.6857625186443329\n",
      "Epoch 0, step 300 - Total_idx 6 - Train Loss: 0.6862733674049377 - Test Loss: 0.6842571794986725\n",
      "Epoch 0, step 350 - Total_idx 7 - Train Loss: 0.6820204842090607 - Test Loss: 0.6841040313243866\n",
      "Epoch 0, step 400 - Total_idx 8 - Train Loss: 0.6777287745475769 - Test Loss: 0.673362535238266\n",
      "Epoch 0, step 450 - Total_idx 9 - Train Loss: 0.6730448853969574 - Test Loss: 0.6724963068962098\n",
      "Epoch 0, step 500 - Total_idx 10 - Train Loss: 0.6681469452381134 - Test Loss: 0.6667636156082153\n",
      "Epoch 0, step 550 - Total_idx 11 - Train Loss: 0.6562675678730011 - Test Loss: 0.6525564789772034\n",
      "Epoch 0, step 600 - Total_idx 12 - Train Loss: 0.6413567054271698 - Test Loss: 0.6339418709278106\n",
      "Epoch 0, step 650 - Total_idx 13 - Train Loss: 0.6168903756141663 - Test Loss: 0.6058968544006348\n",
      "Epoch 0, step 700 - Total_idx 14 - Train Loss: 0.5835113191604614 - Test Loss: 0.5693433821201325\n",
      "Epoch 0, step 750 - Total_idx 15 - Train Loss: 0.5387563782930375 - Test Loss: 0.5212965965270996\n",
      "Epoch 0, step 800 - Total_idx 16 - Train Loss: 0.5124294191598893 - Test Loss: 0.4887759566307068\n",
      "Epoch 0, step 850 - Total_idx 17 - Train Loss: 0.46397375226020815 - Test Loss: 0.4454866975545883\n",
      "Epoch 0, step 900 - Total_idx 18 - Train Loss: 0.4258184117078781 - Test Loss: 0.3907234489917755\n",
      "Epoch 0, step 950 - Total_idx 19 - Train Loss: 0.4027076148986816 - Test Loss: 0.3980705767869949\n",
      "Epoch 0, step 1000 - Total_idx 20 - Train Loss: 0.3663001948595047 - Test Loss: 0.32509306371212005\n",
      "Epoch 0, step 1050 - Total_idx 21 - Train Loss: 0.3582938599586487 - Test Loss: 0.35404504239559176\n",
      "Epoch 0, step 1100 - Total_idx 22 - Train Loss: 0.3423025307059288 - Test Loss: 0.34302303940057755\n",
      "Epoch 0, step 1150 - Total_idx 23 - Train Loss: 0.32769153505563736 - Test Loss: 0.3325654253363609\n",
      "Epoch 0, step 1200 - Total_idx 24 - Train Loss: 0.30006348103284836 - Test Loss: 0.29968993067741395\n",
      "Epoch 0, step 1250 - Total_idx 25 - Train Loss: 0.3044577777385712 - Test Loss: 0.2621018886566162\n",
      "Epoch 0, step 1300 - Total_idx 26 - Train Loss: 0.28427322566509244 - Test Loss: 0.24196061342954636\n",
      "Epoch 0, step 1350 - Total_idx 27 - Train Loss: 0.2552852717041969 - Test Loss: 0.28445698618888854\n",
      "Epoch 0, step 1400 - Total_idx 28 - Train Loss: 0.26550112158060074 - Test Loss: 0.21332045048475265\n",
      "Epoch 0, step 1450 - Total_idx 29 - Train Loss: 0.2522030249238014 - Test Loss: 0.2952105328440666\n",
      "Epoch 0, step 1500 - Total_idx 30 - Train Loss: 0.2487814462184906 - Test Loss: 0.1925404742360115\n",
      "Epoch 0, step 1550 - Total_idx 31 - Train Loss: 0.23843125998973846 - Test Loss: 0.2763891890645027\n",
      "Epoch 0, step 1600 - Total_idx 32 - Train Loss: 0.22284072279930114 - Test Loss: 0.20040268152952195\n",
      "Epoch 0, step 1650 - Total_idx 33 - Train Loss: 0.20645858943462372 - Test Loss: 0.20588725060224533\n",
      "Epoch 0, step 1700 - Total_idx 34 - Train Loss: 0.24114727407693862 - Test Loss: 0.3109601825475693\n",
      "Epoch 0, step 1750 - Total_idx 35 - Train Loss: 0.21863483399152756 - Test Loss: 0.1670420691370964\n",
      "Epoch 0, step 1800 - Total_idx 36 - Train Loss: 0.23527406617999078 - Test Loss: 0.16160224229097367\n",
      "Epoch 0, step 1850 - Total_idx 37 - Train Loss: 0.19696367725729944 - Test Loss: 0.22846409380435945\n",
      "Epoch 0, step 1900 - Total_idx 38 - Train Loss: 0.22392602235078812 - Test Loss: 0.1776852659881115\n",
      "Epoch 0, step 1950 - Total_idx 39 - Train Loss: 0.22613467678427696 - Test Loss: 0.1857377327978611\n",
      "Epoch 0, step 2000 - Total_idx 40 - Train Loss: 0.1934737491607666 - Test Loss: 0.16268571242690086\n",
      "Epoch 0, step 2050 - Total_idx 41 - Train Loss: 0.2344713716208935 - Test Loss: 0.2331753484904766\n",
      "Epoch 0, step 2100 - Total_idx 42 - Train Loss: 0.20663382709026337 - Test Loss: 0.19316475391387938\n",
      "Epoch 0, step 2150 - Total_idx 43 - Train Loss: 0.19032534435391427 - Test Loss: 0.19703256487846374\n",
      "Epoch 0, step 2200 - Total_idx 44 - Train Loss: 0.20799458160996437 - Test Loss: 0.16522225216031075\n",
      "Epoch 0, step 2250 - Total_idx 45 - Train Loss: 0.20741678953170775 - Test Loss: 0.23507382199168206\n",
      "Epoch 0, step 2300 - Total_idx 46 - Train Loss: 0.19581271350383758 - Test Loss: 0.2121681958436966\n",
      "Epoch 0, step 2350 - Total_idx 47 - Train Loss: 0.2030131359398365 - Test Loss: 0.2300255037844181\n",
      "Epoch 0, step 2400 - Total_idx 48 - Train Loss: 0.1889145392179489 - Test Loss: 0.2145609475672245\n",
      "Epoch 0, step 2450 - Total_idx 49 - Train Loss: 0.19928191125392913 - Test Loss: 0.2172978326678276\n",
      "Epoch 0, step 2500 - Total_idx 50 - Train Loss: 0.19147181004285813 - Test Loss: 0.172994926571846\n",
      "Epoch 0, step 2550 - Total_idx 51 - Train Loss: 0.21299990013241768 - Test Loss: 0.2152402363717556\n",
      "Epoch 0, step 2600 - Total_idx 52 - Train Loss: 0.181696038544178 - Test Loss: 0.22408279478549958\n",
      "Epoch 0, step 2650 - Total_idx 53 - Train Loss: 0.15143947690725326 - Test Loss: 0.21029265895485877\n",
      "Epoch 0, step 2700 - Total_idx 54 - Train Loss: 0.17854504108428956 - Test Loss: 0.15333814173936844\n",
      "Epoch 0, step 2750 - Total_idx 55 - Train Loss: 0.1783343204855919 - Test Loss: 0.15120624750852585\n",
      "Epoch 0, step 2800 - Total_idx 56 - Train Loss: 0.15690906465053558 - Test Loss: 0.19680776223540306\n",
      "Epoch 0, step 2850 - Total_idx 57 - Train Loss: 0.18870619177818299 - Test Loss: 0.15601440668106079\n",
      "Epoch 0, step 2900 - Total_idx 58 - Train Loss: 0.18323407784104348 - Test Loss: 0.1333206996321678\n",
      "Epoch 0, step 2950 - Total_idx 59 - Train Loss: 0.2154478296637535 - Test Loss: 0.2050461709499359\n",
      "Epoch 0, step 3000 - Total_idx 60 - Train Loss: 0.17985613495111466 - Test Loss: 0.20468854457139968\n",
      "Epoch 0, step 3050 - Total_idx 61 - Train Loss: 0.17403528913855554 - Test Loss: 0.13940679132938386\n",
      "Epoch 0, step 3100 - Total_idx 62 - Train Loss: 0.18089128389954567 - Test Loss: 0.24146474599838258\n",
      "Epoch 0, step 3150 - Total_idx 63 - Train Loss: 0.16385684229433536 - Test Loss: 0.1981313556432724\n",
      "Epoch 0, step 3200 - Total_idx 64 - Train Loss: 0.15995961740612985 - Test Loss: 0.16501860544085503\n",
      "Epoch 0, step 3250 - Total_idx 65 - Train Loss: 0.15920629665255548 - Test Loss: 0.11103970855474472\n",
      "Epoch 0, step 3300 - Total_idx 66 - Train Loss: 0.15459457233548166 - Test Loss: 0.1634918127208948\n",
      "Epoch 0, step 3350 - Total_idx 67 - Train Loss: 0.17948782585561276 - Test Loss: 0.14601634964346885\n",
      "Epoch 0, step 3400 - Total_idx 68 - Train Loss: 0.12477091819047928 - Test Loss: 0.17579202875494956\n",
      "Epoch 0, step 3450 - Total_idx 69 - Train Loss: 0.17229844212532044 - Test Loss: 0.18817434571683406\n",
      "Epoch 0, step 3500 - Total_idx 70 - Train Loss: 0.18787486799061298 - Test Loss: 0.1682588130235672\n",
      "Epoch 0, step 3550 - Total_idx 71 - Train Loss: 0.16129323810338975 - Test Loss: 0.12742486260831357\n",
      "Epoch 0, step 3600 - Total_idx 72 - Train Loss: 0.14028538435697555 - Test Loss: 0.16581316329538823\n",
      "Epoch 0, step 3650 - Total_idx 73 - Train Loss: 0.15496289417147635 - Test Loss: 0.13499111980199813\n",
      "Epoch 0, step 3700 - Total_idx 74 - Train Loss: 0.15358090214431286 - Test Loss: 0.10007423013448716\n",
      "Epoch 0, step 3750 - Total_idx 75 - Train Loss: 0.14690094046294688 - Test Loss: 0.15999161154031755\n",
      "Epoch 0, step 3800 - Total_idx 76 - Train Loss: 0.1584128287434578 - Test Loss: 0.142507279291749\n",
      "Epoch 0, step 3850 - Total_idx 77 - Train Loss: 0.1596902172267437 - Test Loss: 0.1475883934646845\n",
      "Epoch 0, step 3900 - Total_idx 78 - Train Loss: 0.18745466858148574 - Test Loss: 0.1979955691844225\n",
      "Epoch 0, step 3950 - Total_idx 79 - Train Loss: 0.1697232051938772 - Test Loss: 0.10997029207646847\n",
      "Epoch 0, step 4000 - Total_idx 80 - Train Loss: 0.14025539971888065 - Test Loss: 0.1330939095467329\n",
      "Epoch 0, step 4050 - Total_idx 81 - Train Loss: 0.14716782569885253 - Test Loss: 0.1602811101824045\n",
      "Epoch 0, step 4100 - Total_idx 82 - Train Loss: 0.1637301690876484 - Test Loss: 0.12273723147809505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 4150 - Total_idx 83 - Train Loss: 0.15225682988762856 - Test Loss: 0.14154745563864707\n",
      "Epoch 0, step 4200 - Total_idx 84 - Train Loss: 0.16119916677474977 - Test Loss: 0.17531685754656792\n",
      "Epoch 0, step 4250 - Total_idx 85 - Train Loss: 0.1707561109215021 - Test Loss: 0.12810737378895282\n",
      "Epoch 0, step 4300 - Total_idx 86 - Train Loss: 0.18483910992741584 - Test Loss: 0.18179333880543708\n",
      "Epoch 0, step 4350 - Total_idx 87 - Train Loss: 0.1691689371317625 - Test Loss: 0.1808835469186306\n",
      "Epoch 0, step 4400 - Total_idx 88 - Train Loss: 0.1423685935139656 - Test Loss: 0.18879357874393463\n",
      "Epoch 0, step 4450 - Total_idx 89 - Train Loss: 0.1787788115441799 - Test Loss: 0.06649012602865696\n",
      "Epoch 0, step 4500 - Total_idx 90 - Train Loss: 0.130839421749115 - Test Loss: 0.15027653463184834\n",
      "Epoch 0, step 4550 - Total_idx 91 - Train Loss: 0.1631448559463024 - Test Loss: 0.16242379024624826\n",
      "Epoch 0, step 4600 - Total_idx 92 - Train Loss: 0.13240697786211966 - Test Loss: 0.2051428534090519\n",
      "Epoch 0, step 4650 - Total_idx 93 - Train Loss: 0.1697839942574501 - Test Loss: 0.16709027513861657\n",
      "Epoch 0, step 4700 - Total_idx 94 - Train Loss: 0.133996649235487 - Test Loss: 0.18328131623566152\n",
      "Epoch 0, step 4750 - Total_idx 95 - Train Loss: 0.19239404618740083 - Test Loss: 0.1152106523513794\n",
      "Epoch 0, step 4800 - Total_idx 96 - Train Loss: 0.17771408148109913 - Test Loss: 0.1407073110342026\n",
      "Epoch 0, step 4850 - Total_idx 97 - Train Loss: 0.13845796562731266 - Test Loss: 0.0990857757627964\n",
      "Epoch 0, step 4900 - Total_idx 98 - Train Loss: 0.14671558976173402 - Test Loss: 0.2035288531333208\n",
      "Epoch 0, step 4950 - Total_idx 99 - Train Loss: 0.15140624023973942 - Test Loss: 0.1043266385793686\n",
      "Epoch 0, step 5000 - Total_idx 100 - Train Loss: 0.13875738136470317 - Test Loss: 0.11624128371477127\n",
      "Epoch 0, step 5050 - Total_idx 101 - Train Loss: 0.15326357990503311 - Test Loss: 0.1510500494390726\n",
      "Epoch 0, step 5100 - Total_idx 102 - Train Loss: 0.13559604302048683 - Test Loss: 0.08076327815651893\n",
      "Epoch 0, step 5150 - Total_idx 103 - Train Loss: 0.12499205090105533 - Test Loss: 0.07385014928877354\n",
      "Epoch 0, step 5200 - Total_idx 104 - Train Loss: 0.15793920941650869 - Test Loss: 0.18400756306946278\n",
      "Epoch 0, step 5250 - Total_idx 105 - Train Loss: 0.14141806587576866 - Test Loss: 0.14785537049174308\n",
      "Epoch 0, step 5300 - Total_idx 106 - Train Loss: 0.17257346265017987 - Test Loss: 0.14532292000949382\n",
      "Epoch 0, step 5350 - Total_idx 107 - Train Loss: 0.15779141180217265 - Test Loss: 0.14627395197749138\n",
      "Epoch 0, step 5400 - Total_idx 108 - Train Loss: 0.1511955250054598 - Test Loss: 0.1286116637289524\n",
      "Epoch 0, step 5450 - Total_idx 109 - Train Loss: 0.12937499903142452 - Test Loss: 0.13050454258918762\n",
      "Epoch 0, step 5500 - Total_idx 110 - Train Loss: 0.1162609200924635 - Test Loss: 0.13115559071302413\n",
      "Epoch 0, step 5550 - Total_idx 111 - Train Loss: 0.1270706494152546 - Test Loss: 0.10717335939407349\n",
      "Epoch 0, step 5600 - Total_idx 112 - Train Loss: 0.13864608436822892 - Test Loss: 0.14688497707247733\n",
      "Epoch 0, step 5650 - Total_idx 113 - Train Loss: 0.14477427024394274 - Test Loss: 0.07680786065757275\n",
      "Epoch 0, step 5700 - Total_idx 114 - Train Loss: 0.11597513258457184 - Test Loss: 0.11317918617278337\n",
      "Epoch 0, step 5750 - Total_idx 115 - Train Loss: 0.14261789418756962 - Test Loss: 0.12300134636461735\n",
      "Epoch 0, step 5800 - Total_idx 116 - Train Loss: 0.12486022844910621 - Test Loss: 0.11238410584628582\n",
      "Epoch 0, step 5850 - Total_idx 117 - Train Loss: 0.12516258135437966 - Test Loss: 0.155992865934968\n",
      "Epoch 0, step 5900 - Total_idx 118 - Train Loss: 0.16436434373259545 - Test Loss: 0.1174150787293911\n",
      "Epoch 0, step 5950 - Total_idx 119 - Train Loss: 0.142859212346375 - Test Loss: 0.1393721217289567\n",
      "Epoch 0, step 6000 - Total_idx 120 - Train Loss: 0.14343755695968866 - Test Loss: 0.19981829933822154\n",
      "Epoch 0, step 6050 - Total_idx 121 - Train Loss: 0.14635883182287215 - Test Loss: 0.17611366808414458\n",
      "Epoch 0, step 6100 - Total_idx 122 - Train Loss: 0.11348203066736459 - Test Loss: 0.111453103646636\n",
      "Epoch 0, step 6150 - Total_idx 123 - Train Loss: 0.1399501598626375 - Test Loss: 0.13926213793456554\n",
      "Epoch 0, step 6200 - Total_idx 124 - Train Loss: 0.14362655609846114 - Test Loss: 0.1266954390332103\n",
      "Epoch 0, step 6250 - Total_idx 125 - Train Loss: 0.14694423232227563 - Test Loss: 0.08077306002378463\n",
      "Epoch 0, step 6300 - Total_idx 126 - Train Loss: 0.10953439623117447 - Test Loss: 0.22263142373412848\n",
      "Epoch 0, step 6350 - Total_idx 127 - Train Loss: 0.10371197666972876 - Test Loss: 0.10575628578662873\n",
      "Epoch 0, step 6400 - Total_idx 128 - Train Loss: 0.14918677523732185 - Test Loss: 0.09489125646650791\n",
      "Epoch 0, step 6450 - Total_idx 129 - Train Loss: 0.09986708298325539 - Test Loss: 0.10500912666320801\n",
      "Epoch 0, step 6500 - Total_idx 130 - Train Loss: 0.11514664106070996 - Test Loss: 0.13239174541085957\n",
      "Epoch 0, step 6550 - Total_idx 131 - Train Loss: 0.1260273364931345 - Test Loss: 0.17985871192067862\n",
      "Epoch 0, step 6600 - Total_idx 132 - Train Loss: 0.1318909493088722 - Test Loss: 0.11490729302167893\n",
      "Epoch 0, step 6650 - Total_idx 133 - Train Loss: 0.11923478029668332 - Test Loss: 0.20980592705309392\n",
      "Epoch 0, step 6700 - Total_idx 134 - Train Loss: 0.13247767362743615 - Test Loss: 0.06360264178365468\n",
      "Epoch 0, step 6750 - Total_idx 135 - Train Loss: 0.0964599784836173 - Test Loss: 0.14823593907058238\n",
      "Epoch 0, step 6800 - Total_idx 136 - Train Loss: 0.13512297980487348 - Test Loss: 0.1539235007017851\n",
      "Epoch 0, step 6850 - Total_idx 137 - Train Loss: 0.15053381584584713 - Test Loss: 0.15192606765776873\n",
      "Epoch 0, step 6900 - Total_idx 138 - Train Loss: 0.1382486802712083 - Test Loss: 0.1726661741733551\n",
      "Epoch 0, step 6950 - Total_idx 139 - Train Loss: 0.1189772866666317 - Test Loss: 0.12864121943712234\n",
      "Epoch 0, step 7000 - Total_idx 140 - Train Loss: 0.11240792822092771 - Test Loss: 0.11499776877462864\n",
      "Epoch 0, step 7050 - Total_idx 141 - Train Loss: 0.15841221660375596 - Test Loss: 0.09304627068340779\n",
      "Epoch 0, step 7100 - Total_idx 142 - Train Loss: 0.12171835333108902 - Test Loss: 0.09487840235233307\n",
      "Epoch 0, step 7150 - Total_idx 143 - Train Loss: 0.13357972353696823 - Test Loss: 0.12474347166717052\n",
      "Epoch 0, step 7200 - Total_idx 144 - Train Loss: 0.08289068963378668 - Test Loss: 0.18209623973816633\n",
      "Epoch 0, step 7250 - Total_idx 145 - Train Loss: 0.10510785028338432 - Test Loss: 0.0789221314713359\n",
      "Epoch 0, step 7300 - Total_idx 146 - Train Loss: 0.131972923502326 - Test Loss: 0.1270981639623642\n",
      "Epoch 0, step 7350 - Total_idx 147 - Train Loss: 0.13397445272654296 - Test Loss: 0.07192151769995689\n",
      "Epoch 0, step 7400 - Total_idx 148 - Train Loss: 0.17472990635782482 - Test Loss: 0.17023744713515043\n",
      "Epoch 0, step 7450 - Total_idx 149 - Train Loss: 0.12532875310629607 - Test Loss: 0.10895213149487973\n",
      "Epoch 0, step 7500 - Total_idx 150 - Train Loss: 0.11488647036254405 - Test Loss: 0.1081451777368784\n",
      "Epoch 0, step 7550 - Total_idx 151 - Train Loss: 0.09431707236915826 - Test Loss: 0.11486622337251902\n",
      "Epoch 0, step 7600 - Total_idx 152 - Train Loss: 0.12703580427914857 - Test Loss: 0.1395575750619173\n",
      "Epoch 0, step 7650 - Total_idx 153 - Train Loss: 0.13334271896630526 - Test Loss: 0.16280394680798055\n",
      "Epoch 0, step 7700 - Total_idx 154 - Train Loss: 0.09998251978307962 - Test Loss: 0.14059427734464408\n",
      "Epoch 0, step 7750 - Total_idx 155 - Train Loss: 0.11422909244894981 - Test Loss: 0.14558108393102884\n",
      "Epoch 0, step 7800 - Total_idx 156 - Train Loss: 0.13043847784399987 - Test Loss: 0.08690032232552766\n",
      "Epoch 0, step 7850 - Total_idx 157 - Train Loss: 0.14466188032180072 - Test Loss: 0.15354753322899342\n",
      "Epoch 0, step 7900 - Total_idx 158 - Train Loss: 0.13581067845225334 - Test Loss: 0.14557889103889465\n",
      "Epoch 0, step 7950 - Total_idx 159 - Train Loss: 0.12885321140289308 - Test Loss: 0.1116528807207942\n",
      "Epoch 0, step 8000 - Total_idx 160 - Train Loss: 0.117605659365654 - Test Loss: 0.12759638000279666\n",
      "Epoch 0, step 8050 - Total_idx 161 - Train Loss: 0.13568294387310742 - Test Loss: 0.09213372133672237\n",
      "Epoch 0, step 8100 - Total_idx 162 - Train Loss: 0.15309469666332007 - Test Loss: 0.07817589156329632\n",
      "Epoch 0, step 8150 - Total_idx 163 - Train Loss: 0.1303125074878335 - Test Loss: 0.17067145574837922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 8200 - Total_idx 164 - Train Loss: 0.11806918315589428 - Test Loss: 0.19723746478557586\n",
      "Epoch 0, step 8250 - Total_idx 165 - Train Loss: 0.15126104168593885 - Test Loss: 0.12363320980221033\n",
      "Epoch 0, step 8300 - Total_idx 166 - Train Loss: 0.13819729566574096 - Test Loss: 0.14442144576460123\n",
      "Epoch 0, step 8350 - Total_idx 167 - Train Loss: 0.14144709907472133 - Test Loss: 0.1274049509316683\n",
      "Epoch 0, step 8400 - Total_idx 168 - Train Loss: 0.13881235394626856 - Test Loss: 0.10815760772675276\n",
      "Epoch 0, step 8450 - Total_idx 169 - Train Loss: 0.11198094882071018 - Test Loss: 0.14136933926492928\n",
      "Epoch 0, step 8500 - Total_idx 170 - Train Loss: 0.14794796705245972 - Test Loss: 0.10765183754265309\n",
      "Epoch 0, step 8550 - Total_idx 171 - Train Loss: 0.1115910317748785 - Test Loss: 0.1550994638353586\n",
      "Epoch 0, step 8600 - Total_idx 172 - Train Loss: 0.10016537636518479 - Test Loss: 0.12906459756195546\n",
      "Epoch 0, step 8650 - Total_idx 173 - Train Loss: 0.15584644455462693 - Test Loss: 0.1906270045787096\n",
      "Epoch 0, step 8700 - Total_idx 174 - Train Loss: 0.1557667875289917 - Test Loss: 0.16678189113736153\n",
      "Epoch 0, step 8750 - Total_idx 175 - Train Loss: 0.13352203175425528 - Test Loss: 0.1116489538922906\n",
      "Epoch 0, step 8800 - Total_idx 176 - Train Loss: 0.09103327464312315 - Test Loss: 0.12149183545261621\n",
      "Epoch 0, step 8850 - Total_idx 177 - Train Loss: 0.10165802765637637 - Test Loss: 0.0814056957140565\n",
      "Epoch 0, step 8900 - Total_idx 178 - Train Loss: 0.10934222131967544 - Test Loss: 0.0907778289169073\n",
      "Epoch 0, step 8950 - Total_idx 179 - Train Loss: 0.1244729297235608 - Test Loss: 0.06174879297614098\n",
      "Epoch 0, step 9000 - Total_idx 180 - Train Loss: 0.14140691876411438 - Test Loss: 0.1516608903184533\n",
      "Epoch 0, step 9050 - Total_idx 181 - Train Loss: 0.15503200389444827 - Test Loss: 0.12533044628798962\n",
      "Epoch 0, step 9100 - Total_idx 182 - Train Loss: 0.14741311714053154 - Test Loss: 0.08209429029375315\n",
      "Epoch 0, step 9150 - Total_idx 183 - Train Loss: 0.11994313437491655 - Test Loss: 0.045765820518136026\n",
      "Epoch 0, step 9200 - Total_idx 184 - Train Loss: 0.10848792511969804 - Test Loss: 0.17400276940315962\n",
      "Epoch 0, step 9250 - Total_idx 185 - Train Loss: 0.11418286506086588 - Test Loss: 0.0998687231913209\n",
      "Epoch 0, step 9300 - Total_idx 186 - Train Loss: 0.14378873068839312 - Test Loss: 0.0935307702049613\n",
      "Epoch 0, step 9350 - Total_idx 187 - Train Loss: 0.08456538833677768 - Test Loss: 0.18251293208450078\n",
      "Epoch 0, step 9400 - Total_idx 188 - Train Loss: 0.09858207300305366 - Test Loss: 0.09990205410867929\n",
      "Epoch 0, step 9450 - Total_idx 189 - Train Loss: 0.10411992408335209 - Test Loss: 0.08953883145004511\n",
      "Epoch 0, step 9500 - Total_idx 190 - Train Loss: 0.15318379424512385 - Test Loss: 0.16869769543409346\n",
      "Epoch 0, step 9550 - Total_idx 191 - Train Loss: 0.10915812455117703 - Test Loss: 0.09269218575209379\n",
      "Epoch 0, step 9600 - Total_idx 192 - Train Loss: 0.12668922774493693 - Test Loss: 0.09204324372112752\n",
      "Epoch 0, step 9650 - Total_idx 193 - Train Loss: 0.13752166356891393 - Test Loss: 0.13081098794937135\n",
      "Epoch 0, step 9700 - Total_idx 194 - Train Loss: 0.13812218967825174 - Test Loss: 0.1177303571254015\n",
      "Epoch 0, step 9750 - Total_idx 195 - Train Loss: 0.14416385784745217 - Test Loss: 0.12768047042191027\n",
      "Epoch 0, step 9800 - Total_idx 196 - Train Loss: 0.11330159407109022 - Test Loss: 0.10769922882318497\n",
      "Epoch 0, step 9850 - Total_idx 197 - Train Loss: 0.11759956303983926 - Test Loss: 0.07056904714554549\n",
      "Epoch 0, step 9900 - Total_idx 198 - Train Loss: 0.10278039298951626 - Test Loss: 0.17343413792550563\n",
      "Epoch 0, step 9950 - Total_idx 199 - Train Loss: 0.0999379013851285 - Test Loss: 0.14914264623075724\n",
      "Epoch 0, step 10000 - Total_idx 200 - Train Loss: 0.14538972560316324 - Test Loss: 0.13752257078886032\n",
      "Epoch 0, step 10050 - Total_idx 201 - Train Loss: 0.10728876233100891 - Test Loss: 0.09208557894453406\n",
      "Epoch 0, step 10100 - Total_idx 202 - Train Loss: 0.12162908244878054 - Test Loss: 0.14065156690776348\n",
      "Epoch 0, step 10150 - Total_idx 203 - Train Loss: 0.1191834243759513 - Test Loss: 0.04728919398039579\n",
      "Epoch 0, step 10200 - Total_idx 204 - Train Loss: 0.11283159621059895 - Test Loss: 0.17036971114575863\n",
      "Epoch 0, step 10250 - Total_idx 205 - Train Loss: 0.09913347102701664 - Test Loss: 0.15281273536384105\n",
      "Epoch 0, step 10300 - Total_idx 206 - Train Loss: 0.12612805843353272 - Test Loss: 0.17499793535098435\n",
      "Epoch 0, step 10350 - Total_idx 207 - Train Loss: 0.09261525535956025 - Test Loss: 0.14558251798152924\n",
      "Epoch 0, step 10400 - Total_idx 208 - Train Loss: 0.12838466051965952 - Test Loss: 0.19114196952432394\n",
      "Epoch 0, step 10450 - Total_idx 209 - Train Loss: 0.1338403965719044 - Test Loss: 0.1258563815616071\n",
      "Epoch 0, step 10500 - Total_idx 210 - Train Loss: 0.11025057168677449 - Test Loss: 0.05332347396761179\n",
      "Epoch 0, step 10550 - Total_idx 211 - Train Loss: 0.1308888802677393 - Test Loss: 0.09658510610461235\n",
      "Epoch 0, step 10600 - Total_idx 212 - Train Loss: 0.11642193332314492 - Test Loss: 0.138199557736516\n",
      "Epoch 0, step 10650 - Total_idx 213 - Train Loss: 0.1482376785390079 - Test Loss: 0.17314999718219043\n",
      "Epoch 0, step 10700 - Total_idx 214 - Train Loss: 0.15728360000997782 - Test Loss: 0.07254521651193499\n",
      "Epoch 0, step 10750 - Total_idx 215 - Train Loss: 0.119872359149158 - Test Loss: 0.07454723380506038\n",
      "Epoch 0, step 10800 - Total_idx 216 - Train Loss: 0.13266204621642827 - Test Loss: 0.127036770619452\n",
      "Epoch 0, step 10850 - Total_idx 217 - Train Loss: 0.1289246867597103 - Test Loss: 0.07535696811974049\n",
      "Epoch 0, step 10900 - Total_idx 218 - Train Loss: 0.13326810035854578 - Test Loss: 0.11162544041872025\n",
      "Epoch 0, step 10950 - Total_idx 219 - Train Loss: 0.1123031059652567 - Test Loss: 0.12656485419720412\n",
      "Epoch 0, step 11000 - Total_idx 220 - Train Loss: 0.10055243140086531 - Test Loss: 0.12491931840777397\n",
      "Epoch 0, step 11050 - Total_idx 221 - Train Loss: 0.12564449049532414 - Test Loss: 0.11770158372819424\n",
      "Epoch 0, step 11100 - Total_idx 222 - Train Loss: 0.11269905984401703 - Test Loss: 0.10327660627663135\n",
      "Epoch 0, step 11150 - Total_idx 223 - Train Loss: 0.15466624975204468 - Test Loss: 0.09193340670317411\n",
      "Epoch 0, step 11200 - Total_idx 224 - Train Loss: 0.11816136617213488 - Test Loss: 0.131719383969903\n",
      "Epoch 0, step 11250 - Total_idx 225 - Train Loss: 0.07169676471501589 - Test Loss: 0.09281967170536518\n",
      "Epoch 0, step 11300 - Total_idx 226 - Train Loss: 0.08739003082737326 - Test Loss: 0.12415438937023282\n",
      "Epoch 0, step 11350 - Total_idx 227 - Train Loss: 0.09956931890919804 - Test Loss: 0.10926555562764406\n",
      "Epoch 0, step 11400 - Total_idx 228 - Train Loss: 0.12146720288321376 - Test Loss: 0.09637470180168747\n",
      "Epoch 0, step 11450 - Total_idx 229 - Train Loss: 0.11657559547573328 - Test Loss: 0.06247156169265509\n",
      "Epoch 0, step 11500 - Total_idx 230 - Train Loss: 0.10341189948841929 - Test Loss: 0.10545360865071415\n",
      "Epoch 0, step 11550 - Total_idx 231 - Train Loss: 0.08460526123642921 - Test Loss: 0.05954328561201692\n",
      "Epoch 0, step 11600 - Total_idx 232 - Train Loss: 0.09145089201629161 - Test Loss: 0.04955212865024805\n",
      "Epoch 0, step 11650 - Total_idx 233 - Train Loss: 0.11909020205959678 - Test Loss: 0.11958491057157516\n",
      "Epoch 0, step 11700 - Total_idx 234 - Train Loss: 0.1065532136708498 - Test Loss: 0.10714765544980764\n",
      "Epoch 0, step 11750 - Total_idx 235 - Train Loss: 0.13997031973674892 - Test Loss: 0.15395180629566313\n",
      "Epoch 0, step 11800 - Total_idx 236 - Train Loss: 0.12966561974957586 - Test Loss: 0.07958574648946523\n",
      "Epoch 0, step 11850 - Total_idx 237 - Train Loss: 0.09413186935707926 - Test Loss: 0.14707682440057396\n",
      "Epoch 0, step 11900 - Total_idx 238 - Train Loss: 0.10464389096945524 - Test Loss: 0.22845187019556762\n",
      "Epoch 0, step 11950 - Total_idx 239 - Train Loss: 0.1482311590015888 - Test Loss: 0.1441632304340601\n",
      "Epoch 0, step 12000 - Total_idx 240 - Train Loss: 0.12040117293596268 - Test Loss: 0.1053050913847983\n",
      "Epoch 0, step 12050 - Total_idx 241 - Train Loss: 0.11635899214074015 - Test Loss: 0.045776144973933695\n",
      "Epoch 0, step 12100 - Total_idx 242 - Train Loss: 0.10995296774432063 - Test Loss: 0.15545861441642045\n",
      "Epoch 0, step 12150 - Total_idx 243 - Train Loss: 0.15857637768611313 - Test Loss: 0.06531529892235995\n",
      "Epoch 0, step 12200 - Total_idx 244 - Train Loss: 0.14971489952877165 - Test Loss: 0.09660233836621046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 12250 - Total_idx 245 - Train Loss: 0.10503254113718867 - Test Loss: 0.06719416435807943\n",
      "Epoch 0, step 12300 - Total_idx 246 - Train Loss: 0.11933296702802182 - Test Loss: 0.13252359852194787\n",
      "Epoch 0, step 12350 - Total_idx 247 - Train Loss: 0.10908584417775273 - Test Loss: 0.03895729165524244\n",
      "Epoch 0, step 12400 - Total_idx 248 - Train Loss: 0.11597047636285424 - Test Loss: 0.12410275992006063\n",
      "Epoch 0, step 12450 - Total_idx 249 - Train Loss: 0.14460157316178082 - Test Loss: 0.05672986572608352\n",
      "Epoch 0, step 12500 - Total_idx 250 - Train Loss: 0.10550199996680021 - Test Loss: 0.07097643539309502\n",
      "Epoch 0, step 12550 - Total_idx 251 - Train Loss: 0.1486781185492873 - Test Loss: 0.16181438378989696\n",
      "Epoch 0, step 12600 - Total_idx 252 - Train Loss: 0.10830790733918548 - Test Loss: 0.04202572703361511\n",
      "Epoch 0, step 12650 - Total_idx 253 - Train Loss: 0.10024065675213933 - Test Loss: 0.1435045162215829\n",
      "Epoch 0, step 12700 - Total_idx 254 - Train Loss: 0.1104754725843668 - Test Loss: 0.14439669447019696\n",
      "Epoch 0, step 12750 - Total_idx 255 - Train Loss: 0.14444984996691346 - Test Loss: 0.04017957048490643\n",
      "Epoch 0, step 12800 - Total_idx 256 - Train Loss: 0.11852630820125341 - Test Loss: 0.1473089771345258\n",
      "Epoch 0, step 12850 - Total_idx 257 - Train Loss: 0.10759762220084668 - Test Loss: 0.16784196374937893\n",
      "Epoch 0, step 12900 - Total_idx 258 - Train Loss: 0.1656187544390559 - Test Loss: 0.11821016306057572\n",
      "Epoch 0, step 12950 - Total_idx 259 - Train Loss: 0.11826265962794423 - Test Loss: 0.11236165808513761\n",
      "Epoch 0, step 13000 - Total_idx 260 - Train Loss: 0.13832349110394715 - Test Loss: 0.12985453801229596\n",
      "Epoch 0, step 13050 - Total_idx 261 - Train Loss: 0.12896994307637213 - Test Loss: 0.13752386812120676\n",
      "Epoch 0, step 13100 - Total_idx 262 - Train Loss: 0.12230203507468104 - Test Loss: 0.21560018006712198\n",
      "Epoch 0, step 13150 - Total_idx 263 - Train Loss: 0.1071032247878611 - Test Loss: 0.10395669229328633\n",
      "Epoch 0, step 13200 - Total_idx 264 - Train Loss: 0.1259375311806798 - Test Loss: 0.07670406941324473\n",
      "Epoch 0, step 13250 - Total_idx 265 - Train Loss: 0.08280351724475622 - Test Loss: 0.08541141804307699\n",
      "Epoch 0, step 13300 - Total_idx 266 - Train Loss: 0.12085257722064853 - Test Loss: 0.04920100923627615\n",
      "Epoch 0, step 13350 - Total_idx 267 - Train Loss: 0.11976798379793763 - Test Loss: 0.06669895611703396\n",
      "Epoch 0, step 13400 - Total_idx 268 - Train Loss: 0.1751653239130974 - Test Loss: 0.16222246857360006\n",
      "Epoch 0, step 13450 - Total_idx 269 - Train Loss: 0.11899256482720375 - Test Loss: 0.12698554955422878\n",
      "Epoch 0, step 13500 - Total_idx 270 - Train Loss: 0.1187119023129344 - Test Loss: 0.22543566646054386\n",
      "Epoch 0, step 13550 - Total_idx 271 - Train Loss: 0.10726288681849837 - Test Loss: 0.16975244572386144\n",
      "Epoch 0, step 13600 - Total_idx 272 - Train Loss: 0.09870542723685503 - Test Loss: 0.14649629946798087\n",
      "Epoch 0, step 13650 - Total_idx 273 - Train Loss: 0.11309330966323614 - Test Loss: 0.1249050279147923\n",
      "Epoch 0, step 13700 - Total_idx 274 - Train Loss: 0.13682036995887756 - Test Loss: 0.08931273808702826\n",
      "Epoch 0, step 13750 - Total_idx 275 - Train Loss: 0.14202926322817802 - Test Loss: 0.12817740477621556\n",
      "Epoch 0, step 13800 - Total_idx 276 - Train Loss: 0.14801113182678818 - Test Loss: 0.09304534932598471\n",
      "Epoch 0, step 13850 - Total_idx 277 - Train Loss: 0.12285023737698793 - Test Loss: 0.06644439287483692\n",
      "Epoch 0, step 13900 - Total_idx 278 - Train Loss: 0.09871661491692066 - Test Loss: 0.0691621033474803\n",
      "Epoch 0, step 13950 - Total_idx 279 - Train Loss: 0.11902685245499015 - Test Loss: 0.10893340948969125\n",
      "Epoch 0, step 14000 - Total_idx 280 - Train Loss: 0.10534692415967584 - Test Loss: 0.05929581169039011\n",
      "Epoch 0, step 14050 - Total_idx 281 - Train Loss: 0.11303687585517765 - Test Loss: 0.142731714528054\n",
      "Epoch 0, step 14100 - Total_idx 282 - Train Loss: 0.10462370973080397 - Test Loss: 0.17178262174129486\n",
      "Epoch 0, step 14150 - Total_idx 283 - Train Loss: 0.14423958914354443 - Test Loss: 0.05221119672060013\n",
      "Epoch 0, step 14200 - Total_idx 284 - Train Loss: 0.0940609186142683 - Test Loss: 0.10556160788983107\n",
      "Epoch 0, step 14250 - Total_idx 285 - Train Loss: 0.12522867493331433 - Test Loss: 0.16416312009096146\n",
      "Epoch 0, step 14300 - Total_idx 286 - Train Loss: 0.11078604657202959 - Test Loss: 0.05731482058763504\n",
      "Epoch 0, step 14350 - Total_idx 287 - Train Loss: 0.10869055198505521 - Test Loss: 0.09345838837325574\n",
      "Epoch 0, step 14400 - Total_idx 288 - Train Loss: 0.11164643740281463 - Test Loss: 0.15026095900684594\n",
      "Epoch 0, step 14450 - Total_idx 289 - Train Loss: 0.12509987456724048 - Test Loss: 0.18172222655266523\n",
      "Epoch 0, step 14500 - Total_idx 290 - Train Loss: 0.14463532250374556 - Test Loss: 0.08204592764377594\n",
      "Epoch 0, step 14550 - Total_idx 291 - Train Loss: 0.13466450789943338 - Test Loss: 0.08369330111891031\n",
      "Epoch 0, step 14600 - Total_idx 292 - Train Loss: 0.11231818279251456 - Test Loss: 0.14862255286425352\n",
      "Epoch 0, step 14650 - Total_idx 293 - Train Loss: 0.10344973113387823 - Test Loss: 0.07407224345952272\n",
      "Epoch 0, step 14700 - Total_idx 294 - Train Loss: 0.13006918609142304 - Test Loss: 0.09271175749599933\n",
      "Epoch 0, step 14750 - Total_idx 295 - Train Loss: 0.11112522071227432 - Test Loss: 0.0702621198259294\n",
      "Epoch 0, step 14800 - Total_idx 296 - Train Loss: 0.10085324376821518 - Test Loss: 0.05323753571137786\n",
      "Epoch 0, step 14850 - Total_idx 297 - Train Loss: 0.10566042432561516 - Test Loss: 0.09202970433980226\n",
      "Epoch 0, step 14900 - Total_idx 298 - Train Loss: 0.13520530803129077 - Test Loss: 0.061985491402447225\n",
      "Epoch 0, step 14950 - Total_idx 299 - Train Loss: 0.13567762641236186 - Test Loss: 0.08451522355899214\n",
      "Epoch 0, step 15000 - Total_idx 300 - Train Loss: 0.14577771559357644 - Test Loss: 0.03590069087222218\n",
      "Epoch 0, step 15050 - Total_idx 301 - Train Loss: 0.09965316535905004 - Test Loss: 0.07096160808578134\n",
      "Epoch 0, step 15100 - Total_idx 302 - Train Loss: 0.10795275948941707 - Test Loss: 0.154249544814229\n",
      "Epoch 0, step 15150 - Total_idx 303 - Train Loss: 0.07998135743662715 - Test Loss: 0.09927765801548957\n",
      "Epoch 0, step 15200 - Total_idx 304 - Train Loss: 0.1059645321406424 - Test Loss: 0.0916515606455505\n",
      "Epoch 0, step 15250 - Total_idx 305 - Train Loss: 0.1149050959199667 - Test Loss: 0.062027819082140924\n",
      "Epoch 0, step 15300 - Total_idx 306 - Train Loss: 0.09181216234341263 - Test Loss: 0.11400273945182562\n",
      "Epoch 0, step 15350 - Total_idx 307 - Train Loss: 0.10722379261627793 - Test Loss: 0.09429114628583193\n",
      "Epoch 0, step 15400 - Total_idx 308 - Train Loss: 0.13433434307575226 - Test Loss: 0.09778722506016493\n",
      "Epoch 0, step 15450 - Total_idx 309 - Train Loss: 0.16044586958363652 - Test Loss: 0.06415782878175377\n",
      "Epoch 0, step 15500 - Total_idx 310 - Train Loss: 0.09931016905233264 - Test Loss: 0.07610893286764622\n",
      "Epoch 0, step 15550 - Total_idx 311 - Train Loss: 0.12074950411915779 - Test Loss: 0.08269829675555229\n",
      "Epoch 0, step 15600 - Total_idx 312 - Train Loss: 0.11477993294596672 - Test Loss: 0.08334904313087463\n",
      "Epoch 0, step 15650 - Total_idx 313 - Train Loss: 0.13064273908734322 - Test Loss: 0.15863807108253242\n",
      "Epoch 0, step 15700 - Total_idx 314 - Train Loss: 0.12104449899867177 - Test Loss: 0.07048796350136399\n",
      "Epoch 0, step 15750 - Total_idx 315 - Train Loss: 0.10553699815645814 - Test Loss: 0.05076087508350611\n",
      "Epoch 0, step 15800 - Total_idx 316 - Train Loss: 0.12159179957583546 - Test Loss: 0.09369991384446621\n",
      "Epoch 0, step 15850 - Total_idx 317 - Train Loss: 0.10279427276924252 - Test Loss: 0.08271230403333903\n",
      "Epoch 0, step 15900 - Total_idx 318 - Train Loss: 0.13241399535909296 - Test Loss: 0.08616130296140909\n",
      "Epoch 0, step 15950 - Total_idx 319 - Train Loss: 0.11686897655948997 - Test Loss: 0.12913223821669817\n",
      "\t* ===================================== *\n",
      "\t* Epoch 0 :\n",
      "\t* \t Average Train Loss: 0.17060886366287015\n",
      "\t* \t Average Test Loss: 0.1656379392775125\n",
      "Epoch 1, step 0 - Total_idx 320 - Train Loss: 0.14320607483386993 - Test Loss: 0.16525420965626836\n",
      "Epoch 1, step 50 - Total_idx 321 - Train Loss: 0.10873205911368132 - Test Loss: 0.13246691990643739\n",
      "Epoch 1, step 100 - Total_idx 322 - Train Loss: 0.11181812198832632 - Test Loss: 0.165178853366524\n",
      "Epoch 1, step 150 - Total_idx 323 - Train Loss: 0.12074112618342042 - Test Loss: 0.1272412952966988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 200 - Total_idx 324 - Train Loss: 0.0871542169712484 - Test Loss: 0.1458364199846983\n",
      "Epoch 1, step 250 - Total_idx 325 - Train Loss: 0.09324823157861829 - Test Loss: 0.07943082191050052\n",
      "Epoch 1, step 300 - Total_idx 326 - Train Loss: 0.10113020591437817 - Test Loss: 0.09940218180418015\n",
      "Epoch 1, step 350 - Total_idx 327 - Train Loss: 0.09711156975477934 - Test Loss: 0.0691374959424138\n",
      "Epoch 1, step 400 - Total_idx 328 - Train Loss: 0.10323010969907045 - Test Loss: 0.05951033104211092\n",
      "Epoch 1, step 450 - Total_idx 329 - Train Loss: 0.1154153560474515 - Test Loss: 0.13639904977753758\n",
      "Epoch 1, step 500 - Total_idx 330 - Train Loss: 0.11870615864172578 - Test Loss: 0.09509808421134949\n",
      "Epoch 1, step 550 - Total_idx 331 - Train Loss: 0.1157166251167655 - Test Loss: 0.1107386639341712\n",
      "Epoch 1, step 600 - Total_idx 332 - Train Loss: 0.10427233684808015 - Test Loss: 0.07335998835042119\n",
      "Epoch 1, step 650 - Total_idx 333 - Train Loss: 0.1282779116742313 - Test Loss: 0.1502027679234743\n",
      "Epoch 1, step 700 - Total_idx 334 - Train Loss: 0.10925906769931316 - Test Loss: 0.08533606827259063\n",
      "Epoch 1, step 750 - Total_idx 335 - Train Loss: 0.09973847059533 - Test Loss: 0.12189325522631407\n",
      "Epoch 1, step 800 - Total_idx 336 - Train Loss: 0.0807173160277307 - Test Loss: 0.09461873304098845\n",
      "Epoch 1, step 850 - Total_idx 337 - Train Loss: 0.0847457118704915 - Test Loss: 0.1349984411150217\n",
      "Epoch 1, step 900 - Total_idx 338 - Train Loss: 0.12026756765320896 - Test Loss: 0.061837917845696214\n",
      "Epoch 1, step 950 - Total_idx 339 - Train Loss: 0.08934836130589247 - Test Loss: 0.17175387116149068\n",
      "Epoch 1, step 1000 - Total_idx 340 - Train Loss: 0.09796233983710408 - Test Loss: 0.11900678761303425\n",
      "Epoch 1, step 1050 - Total_idx 341 - Train Loss: 0.10216029872186481 - Test Loss: 0.10858560558408499\n",
      "Epoch 1, step 1100 - Total_idx 342 - Train Loss: 0.11811458194628358 - Test Loss: 0.08949007475748658\n",
      "Epoch 1, step 1150 - Total_idx 343 - Train Loss: 0.13341179300099612 - Test Loss: 0.06871105879545211\n",
      "Epoch 1, step 1200 - Total_idx 344 - Train Loss: 0.11908675622195006 - Test Loss: 0.06747320909053087\n",
      "Epoch 1, step 1250 - Total_idx 345 - Train Loss: 0.09657806208357216 - Test Loss: 0.05891579827293754\n",
      "Epoch 1, step 1300 - Total_idx 346 - Train Loss: 0.094094242695719 - Test Loss: 0.12298590019345283\n",
      "Epoch 1, step 1350 - Total_idx 347 - Train Loss: 0.10563956828787922 - Test Loss: 0.07585646715015174\n",
      "Epoch 1, step 1400 - Total_idx 348 - Train Loss: 0.12399888673797249 - Test Loss: 0.23289210274815558\n",
      "Epoch 1, step 1450 - Total_idx 349 - Train Loss: 0.09534869071096182 - Test Loss: 0.1331155733205378\n",
      "Epoch 1, step 1500 - Total_idx 350 - Train Loss: 0.06879748782142997 - Test Loss: 0.11519065340980887\n",
      "Epoch 1, step 1550 - Total_idx 351 - Train Loss: 0.13001690147444606 - Test Loss: 0.17043199446052312\n",
      "Epoch 1, step 1600 - Total_idx 352 - Train Loss: 0.1269134955853224 - Test Loss: 0.12184863267466425\n",
      "Epoch 1, step 1650 - Total_idx 353 - Train Loss: 0.1329380288720131 - Test Loss: 0.13194800019264222\n",
      "Epoch 1, step 1700 - Total_idx 354 - Train Loss: 0.13014002352952958 - Test Loss: 0.0674094034358859\n",
      "Epoch 1, step 1750 - Total_idx 355 - Train Loss: 0.07842592334374786 - Test Loss: 0.14536189241334796\n",
      "Epoch 1, step 1800 - Total_idx 356 - Train Loss: 0.11407963981851936 - Test Loss: 0.1244187280535698\n",
      "Epoch 1, step 1850 - Total_idx 357 - Train Loss: 0.1041315321996808 - Test Loss: 0.10400855718180538\n",
      "Epoch 1, step 1900 - Total_idx 358 - Train Loss: 0.08651020977646112 - Test Loss: 0.09052869556471706\n",
      "Epoch 1, step 1950 - Total_idx 359 - Train Loss: 0.08849603485316038 - Test Loss: 0.10611892193555832\n",
      "Epoch 1, step 2000 - Total_idx 360 - Train Loss: 0.11351251661777496 - Test Loss: 0.03544330783188343\n",
      "Epoch 1, step 2050 - Total_idx 361 - Train Loss: 0.0973682695440948 - Test Loss: 0.07298954734578729\n",
      "Epoch 1, step 2100 - Total_idx 362 - Train Loss: 0.11323719227686524 - Test Loss: 0.05494109131395817\n",
      "Epoch 1, step 2150 - Total_idx 363 - Train Loss: 0.08807406057603658 - Test Loss: 0.12901863465085625\n",
      "Epoch 1, step 2200 - Total_idx 364 - Train Loss: 0.09256114045158029 - Test Loss: 0.07329070866107941\n",
      "Epoch 1, step 2250 - Total_idx 365 - Train Loss: 0.09040953045710921 - Test Loss: 0.0781681191176176\n",
      "Epoch 1, step 2300 - Total_idx 366 - Train Loss: 0.12316055262461305 - Test Loss: 0.10059047294780613\n",
      "Epoch 1, step 2350 - Total_idx 367 - Train Loss: 0.07004242626950145 - Test Loss: 0.26677689235657454\n",
      "Epoch 1, step 2400 - Total_idx 368 - Train Loss: 0.12934653133153914 - Test Loss: 0.08100570850074292\n",
      "Epoch 1, step 2450 - Total_idx 369 - Train Loss: 0.12072401162236929 - Test Loss: 0.047829325310885906\n",
      "Epoch 1, step 2500 - Total_idx 370 - Train Loss: 0.08363948274403811 - Test Loss: 0.07546041263267397\n",
      "Epoch 1, step 2550 - Total_idx 371 - Train Loss: 0.1347788799367845 - Test Loss: 0.09188438691198826\n",
      "Epoch 1, step 2600 - Total_idx 372 - Train Loss: 0.10037075543776154 - Test Loss: 0.09391266722232103\n",
      "Epoch 1, step 2650 - Total_idx 373 - Train Loss: 0.09452950850129127 - Test Loss: 0.11990077402442693\n",
      "Epoch 1, step 2700 - Total_idx 374 - Train Loss: 0.10290510185062886 - Test Loss: 0.11107601169496775\n",
      "Epoch 1, step 2750 - Total_idx 375 - Train Loss: 0.10979983663186431 - Test Loss: 0.06971227275207639\n",
      "Epoch 1, step 2800 - Total_idx 376 - Train Loss: 0.11018920965492725 - Test Loss: 0.0782292720861733\n",
      "Epoch 1, step 2850 - Total_idx 377 - Train Loss: 0.10822936945594847 - Test Loss: 0.06564488429576158\n",
      "Epoch 1, step 2900 - Total_idx 378 - Train Loss: 0.08818876890465617 - Test Loss: 0.07166959168389439\n",
      "Epoch 1, step 2950 - Total_idx 379 - Train Loss: 0.1052524227835238 - Test Loss: 0.10039115846157073\n",
      "Epoch 1, step 3000 - Total_idx 380 - Train Loss: 0.09505170945078134 - Test Loss: 0.14442736618220806\n",
      "Epoch 1, step 3050 - Total_idx 381 - Train Loss: 0.10164968656376004 - Test Loss: 0.10163047546520829\n",
      "Epoch 1, step 3100 - Total_idx 382 - Train Loss: 0.0876136315986514 - Test Loss: 0.09485656442120671\n",
      "Epoch 1, step 3150 - Total_idx 383 - Train Loss: 0.1369025856629014 - Test Loss: 0.08188740760087967\n",
      "Epoch 1, step 3200 - Total_idx 384 - Train Loss: 0.0976418777089566 - Test Loss: 0.07656346373260021\n",
      "Epoch 1, step 3250 - Total_idx 385 - Train Loss: 0.08794392811134458 - Test Loss: 0.07069467948749661\n",
      "Epoch 1, step 3300 - Total_idx 386 - Train Loss: 0.09422346753999591 - Test Loss: 0.191698794066906\n",
      "Epoch 1, step 3350 - Total_idx 387 - Train Loss: 0.08413854840211571 - Test Loss: 0.10692919734865428\n",
      "Epoch 1, step 3400 - Total_idx 388 - Train Loss: 0.107580532040447 - Test Loss: 0.08963653380051255\n",
      "Epoch 1, step 3450 - Total_idx 389 - Train Loss: 0.08861143535003066 - Test Loss: 0.12088566808961332\n",
      "Epoch 1, step 3500 - Total_idx 390 - Train Loss: 0.09930395344272255 - Test Loss: 0.08520151590928435\n",
      "Epoch 1, step 3550 - Total_idx 391 - Train Loss: 0.104598901104182 - Test Loss: 0.044551527593284845\n",
      "Epoch 1, step 3600 - Total_idx 392 - Train Loss: 0.08708695692941547 - Test Loss: 0.12837524665519595\n",
      "Epoch 1, step 3650 - Total_idx 393 - Train Loss: 0.10399976020678878 - Test Loss: 0.08136387933045626\n",
      "Epoch 1, step 3700 - Total_idx 394 - Train Loss: 0.12769404791295527 - Test Loss: 0.0805367298424244\n",
      "Epoch 1, step 3750 - Total_idx 395 - Train Loss: 0.11717762980610132 - Test Loss: 0.0858812103047967\n",
      "Epoch 1, step 3800 - Total_idx 396 - Train Loss: 0.11298365905880928 - Test Loss: 0.14979511108249427\n",
      "Epoch 1, step 3850 - Total_idx 397 - Train Loss: 0.10782512361183763 - Test Loss: 0.07848356049507857\n",
      "Epoch 1, step 3900 - Total_idx 398 - Train Loss: 0.10258100513368845 - Test Loss: 0.08914118818938732\n",
      "Epoch 1, step 3950 - Total_idx 399 - Train Loss: 0.08666362264193594 - Test Loss: 0.09475352801382542\n",
      "Epoch 1, step 4000 - Total_idx 400 - Train Loss: 0.09813761567696928 - Test Loss: 0.1201077254023403\n",
      "Epoch 1, step 4050 - Total_idx 401 - Train Loss: 0.14689317118376494 - Test Loss: 0.048204897157847884\n",
      "Epoch 1, step 4100 - Total_idx 402 - Train Loss: 0.10856480826623738 - Test Loss: 0.14855578262358904\n",
      "Epoch 1, step 4150 - Total_idx 403 - Train Loss: 0.10806865985505283 - Test Loss: 0.13074090788140894\n",
      "Epoch 1, step 4200 - Total_idx 404 - Train Loss: 0.10721015682443977 - Test Loss: 0.08856313461437822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 4250 - Total_idx 405 - Train Loss: 0.11852244363166392 - Test Loss: 0.07842397298663854\n",
      "Epoch 1, step 4300 - Total_idx 406 - Train Loss: 0.09204784752801061 - Test Loss: 0.09687454188242554\n",
      "Epoch 1, step 4350 - Total_idx 407 - Train Loss: 0.0950699429679662 - Test Loss: 0.060958766797557476\n",
      "Epoch 1, step 4400 - Total_idx 408 - Train Loss: 0.10210875354707241 - Test Loss: 0.09686816036701203\n",
      "Epoch 1, step 4450 - Total_idx 409 - Train Loss: 0.08916358824819326 - Test Loss: 0.032342377584427595\n",
      "Epoch 1, step 4500 - Total_idx 410 - Train Loss: 0.10029561953619122 - Test Loss: 0.1344262129627168\n",
      "Epoch 1, step 4550 - Total_idx 411 - Train Loss: 0.06957866193726658 - Test Loss: 0.09351852261461317\n",
      "Epoch 1, step 4600 - Total_idx 412 - Train Loss: 0.1107651131786406 - Test Loss: 0.07654965696856379\n",
      "Epoch 1, step 4650 - Total_idx 413 - Train Loss: 0.11489826288074255 - Test Loss: 0.041033620666712525\n",
      "Epoch 1, step 4700 - Total_idx 414 - Train Loss: 0.11942910317331552 - Test Loss: 0.10270966729149222\n",
      "Epoch 1, step 4750 - Total_idx 415 - Train Loss: 0.09322677291929722 - Test Loss: 0.09995483197271823\n",
      "Epoch 1, step 4800 - Total_idx 416 - Train Loss: 0.11148667218163609 - Test Loss: 0.0994322075508535\n",
      "Epoch 1, step 4850 - Total_idx 417 - Train Loss: 0.0925077710673213 - Test Loss: 0.11935283113270997\n",
      "Epoch 1, step 4900 - Total_idx 418 - Train Loss: 0.08684136525727809 - Test Loss: 0.08098960677161812\n",
      "Epoch 1, step 4950 - Total_idx 419 - Train Loss: 0.09848316877149045 - Test Loss: 0.14733482245355844\n",
      "Epoch 1, step 5000 - Total_idx 420 - Train Loss: 0.08543736692517996 - Test Loss: 0.0405857237521559\n",
      "Epoch 1, step 5050 - Total_idx 421 - Train Loss: 0.08835318749770522 - Test Loss: 0.2148615112528205\n",
      "Epoch 1, step 5100 - Total_idx 422 - Train Loss: 0.07255628255195916 - Test Loss: 0.10601548487320542\n",
      "Epoch 1, step 5150 - Total_idx 423 - Train Loss: 0.11271327864378691 - Test Loss: 0.13315312908962368\n",
      "Epoch 1, step 5200 - Total_idx 424 - Train Loss: 0.13870279306545855 - Test Loss: 0.11415305733680725\n",
      "Epoch 1, step 5250 - Total_idx 425 - Train Loss: 0.1225954322423786 - Test Loss: 0.1000664240680635\n",
      "Epoch 1, step 5300 - Total_idx 426 - Train Loss: 0.1057994930818677 - Test Loss: 0.0661731444299221\n",
      "Epoch 1, step 5350 - Total_idx 427 - Train Loss: 0.09409769292920828 - Test Loss: 0.169473422691226\n",
      "Epoch 1, step 5400 - Total_idx 428 - Train Loss: 0.09012863739393652 - Test Loss: 0.05577095001935959\n",
      "Epoch 1, step 5450 - Total_idx 429 - Train Loss: 0.12330215465277433 - Test Loss: 0.0832556172274053\n",
      "Epoch 1, step 5500 - Total_idx 430 - Train Loss: 0.09430041836574674 - Test Loss: 0.03748556389473379\n",
      "Epoch 1, step 5550 - Total_idx 431 - Train Loss: 0.1142467529606074 - Test Loss: 0.17004876313731074\n",
      "Epoch 1, step 5600 - Total_idx 432 - Train Loss: 0.08618633382953704 - Test Loss: 0.0801627557259053\n",
      "Epoch 1, step 5650 - Total_idx 433 - Train Loss: 0.09823852883651853 - Test Loss: 0.07963215932250023\n",
      "Epoch 1, step 5700 - Total_idx 434 - Train Loss: 0.11674049568362534 - Test Loss: 0.16956959189847112\n",
      "Epoch 1, step 5750 - Total_idx 435 - Train Loss: 0.11373672479763627 - Test Loss: 0.04960444113239646\n",
      "Epoch 1, step 5800 - Total_idx 436 - Train Loss: 0.10628760483115912 - Test Loss: 0.05077807754278183\n",
      "Epoch 1, step 5850 - Total_idx 437 - Train Loss: 0.13925307953730226 - Test Loss: 0.07004215717315673\n",
      "Epoch 1, step 5900 - Total_idx 438 - Train Loss: 0.10017314305528999 - Test Loss: 0.07243380453437567\n",
      "Epoch 1, step 5950 - Total_idx 439 - Train Loss: 0.12960064695216716 - Test Loss: 0.08315039086155593\n",
      "Epoch 1, step 6000 - Total_idx 440 - Train Loss: 0.10272402044385671 - Test Loss: 0.03968622647225857\n",
      "Epoch 1, step 6050 - Total_idx 441 - Train Loss: 0.09991450283676385 - Test Loss: 0.13594400305300952\n",
      "Epoch 1, step 6100 - Total_idx 442 - Train Loss: 0.07462816187180579 - Test Loss: 0.05917654810473323\n",
      "Epoch 1, step 6150 - Total_idx 443 - Train Loss: 0.09272828140296042 - Test Loss: 0.11261059353128075\n",
      "Epoch 1, step 6200 - Total_idx 444 - Train Loss: 0.08777738939970732 - Test Loss: 0.05901041030883789\n",
      "Epoch 1, step 6250 - Total_idx 445 - Train Loss: 0.09889756295830011 - Test Loss: 0.10300466632470488\n",
      "Epoch 1, step 6300 - Total_idx 446 - Train Loss: 0.09660385448485613 - Test Loss: 0.10542398458346725\n",
      "Epoch 1, step 6350 - Total_idx 447 - Train Loss: 0.10776708212681115 - Test Loss: 0.1311182664707303\n",
      "Epoch 1, step 6400 - Total_idx 448 - Train Loss: 0.11080624939873815 - Test Loss: 0.10054229060187936\n",
      "Epoch 1, step 6450 - Total_idx 449 - Train Loss: 0.12725355854257941 - Test Loss: 0.14489166145212948\n",
      "Epoch 1, step 6500 - Total_idx 450 - Train Loss: 0.12730814926326275 - Test Loss: 0.09074838235974311\n",
      "Epoch 1, step 6550 - Total_idx 451 - Train Loss: 0.10262656882405281 - Test Loss: 0.1343189469538629\n",
      "Epoch 1, step 6600 - Total_idx 452 - Train Loss: 0.09590540634468198 - Test Loss: 0.1392916075885296\n",
      "Epoch 1, step 6650 - Total_idx 453 - Train Loss: 0.07718703957274556 - Test Loss: 0.098109245672822\n",
      "Epoch 1, step 6700 - Total_idx 454 - Train Loss: 0.11740033143199981 - Test Loss: 0.06812016624026\n",
      "Epoch 1, step 6750 - Total_idx 455 - Train Loss: 0.15445201957598328 - Test Loss: 0.0853667383082211\n",
      "Epoch 1, step 6800 - Total_idx 456 - Train Loss: 0.13202497181482614 - Test Loss: 0.13752856887876988\n",
      "Epoch 1, step 6850 - Total_idx 457 - Train Loss: 0.08937904199585318 - Test Loss: 0.07476638816297054\n",
      "Epoch 1, step 6900 - Total_idx 458 - Train Loss: 0.11118986589834094 - Test Loss: 0.05398624446243048\n",
      "Epoch 1, step 6950 - Total_idx 459 - Train Loss: 0.07458526178263128 - Test Loss: 0.1273482064716518\n",
      "Epoch 1, step 7000 - Total_idx 460 - Train Loss: 0.10826385639607906 - Test Loss: 0.18308655507862567\n",
      "Epoch 1, step 7050 - Total_idx 461 - Train Loss: 0.07146154745481909 - Test Loss: 0.0599787344224751\n",
      "Epoch 1, step 7100 - Total_idx 462 - Train Loss: 0.12134745362214744 - Test Loss: 0.15974699500948192\n",
      "Epoch 1, step 7150 - Total_idx 463 - Train Loss: 0.09140089647844434 - Test Loss: 0.13190035349689425\n",
      "Epoch 1, step 7200 - Total_idx 464 - Train Loss: 0.08741204490885139 - Test Loss: 0.1041739889420569\n",
      "Epoch 1, step 7250 - Total_idx 465 - Train Loss: 0.084017364513129 - Test Loss: 0.039742165990173814\n",
      "Epoch 1, step 7300 - Total_idx 466 - Train Loss: 0.11537563146091998 - Test Loss: 0.09355460964143276\n",
      "Epoch 1, step 7350 - Total_idx 467 - Train Loss: 0.09852863600477577 - Test Loss: 0.06141291968524456\n",
      "Epoch 1, step 7400 - Total_idx 468 - Train Loss: 0.11560348577797414 - Test Loss: 0.12060252306982874\n",
      "Epoch 1, step 7450 - Total_idx 469 - Train Loss: 0.11732684450224042 - Test Loss: 0.1061656342819333\n",
      "Epoch 1, step 7500 - Total_idx 470 - Train Loss: 0.06652774683199823 - Test Loss: 0.1425761967431754\n",
      "Epoch 1, step 7550 - Total_idx 471 - Train Loss: 0.11234587508253753 - Test Loss: 0.07428570394404233\n",
      "Epoch 1, step 7600 - Total_idx 472 - Train Loss: 0.09975507955998182 - Test Loss: 0.10474019977264107\n",
      "Epoch 1, step 7650 - Total_idx 473 - Train Loss: 0.09920600092038512 - Test Loss: 0.06535866968333721\n",
      "Epoch 1, step 7700 - Total_idx 474 - Train Loss: 0.10855255246162415 - Test Loss: 0.06650022557005286\n",
      "Epoch 1, step 7750 - Total_idx 475 - Train Loss: 0.08882485409267246 - Test Loss: 0.08629024187102914\n",
      "Epoch 1, step 7800 - Total_idx 476 - Train Loss: 0.10826221374794841 - Test Loss: 0.08902596747502685\n",
      "Epoch 1, step 7850 - Total_idx 477 - Train Loss: 0.11692312973551452 - Test Loss: 0.09557467731647193\n",
      "Epoch 1, step 7900 - Total_idx 478 - Train Loss: 0.10104502408765256 - Test Loss: 0.12065612711012363\n",
      "Epoch 1, step 7950 - Total_idx 479 - Train Loss: 0.1348488407023251 - Test Loss: 0.06387919262051582\n",
      "Epoch 1, step 8000 - Total_idx 480 - Train Loss: 0.09070345174521208 - Test Loss: 0.0685776018537581\n",
      "Epoch 1, step 8050 - Total_idx 481 - Train Loss: 0.12694772573187948 - Test Loss: 0.12378077702596783\n",
      "Epoch 1, step 8100 - Total_idx 482 - Train Loss: 0.0854429729282856 - Test Loss: 0.06631527496501803\n",
      "Epoch 1, step 8150 - Total_idx 483 - Train Loss: 0.0652687604073435 - Test Loss: 0.07820792603306473\n",
      "Epoch 1, step 8200 - Total_idx 484 - Train Loss: 0.11482823991216719 - Test Loss: 0.09230243340134621\n",
      "Epoch 1, step 8250 - Total_idx 485 - Train Loss: 0.10469910318031907 - Test Loss: 0.09152232189662754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 8300 - Total_idx 486 - Train Loss: 0.08984435534104705 - Test Loss: 0.15169991729781032\n",
      "Epoch 1, step 8350 - Total_idx 487 - Train Loss: 0.09965699511580169 - Test Loss: 0.13498407490551473\n",
      "Epoch 1, step 8400 - Total_idx 488 - Train Loss: 0.11163094837218523 - Test Loss: 0.15009334161877633\n",
      "Epoch 1, step 8450 - Total_idx 489 - Train Loss: 0.09866184572689235 - Test Loss: 0.0399047146551311\n",
      "Epoch 1, step 8500 - Total_idx 490 - Train Loss: 0.09589747343212367 - Test Loss: 0.13776588761247693\n",
      "Epoch 1, step 8550 - Total_idx 491 - Train Loss: 0.12728281756862997 - Test Loss: 0.08694090535864234\n",
      "Epoch 1, step 8600 - Total_idx 492 - Train Loss: 0.14671413861215116 - Test Loss: 0.2025694938376546\n",
      "Epoch 1, step 8650 - Total_idx 493 - Train Loss: 0.08221205361187459 - Test Loss: 0.11383000798523427\n",
      "Epoch 1, step 8700 - Total_idx 494 - Train Loss: 0.09715828939341009 - Test Loss: 0.14602843523025513\n",
      "Epoch 1, step 8750 - Total_idx 495 - Train Loss: 0.11790439372882247 - Test Loss: 0.06429255534894765\n",
      "Epoch 1, step 8800 - Total_idx 496 - Train Loss: 0.09898553589358926 - Test Loss: 0.13457284374162554\n",
      "Epoch 1, step 8850 - Total_idx 497 - Train Loss: 0.0767391384486109 - Test Loss: 0.04733368046581745\n",
      "Epoch 1, step 8900 - Total_idx 498 - Train Loss: 0.1186404080875218 - Test Loss: 0.10590034313499927\n",
      "Epoch 1, step 8950 - Total_idx 499 - Train Loss: 0.1285969333909452 - Test Loss: 0.05456337691284716\n",
      "Epoch 1, step 9000 - Total_idx 500 - Train Loss: 0.10795314102433622 - Test Loss: 0.09384467201307416\n",
      "Epoch 1, step 9050 - Total_idx 501 - Train Loss: 0.10232383735477925 - Test Loss: 0.13987732380628587\n",
      "Epoch 1, step 9100 - Total_idx 502 - Train Loss: 0.10607970196753741 - Test Loss: 0.053518340829759835\n",
      "Epoch 1, step 9150 - Total_idx 503 - Train Loss: 0.09761411557905376 - Test Loss: 0.05176561521366239\n",
      "Epoch 1, step 9200 - Total_idx 504 - Train Loss: 0.09795545881614089 - Test Loss: 0.10771985612809658\n",
      "Epoch 1, step 9250 - Total_idx 505 - Train Loss: 0.08137768235057592 - Test Loss: 0.07706543831154704\n",
      "Epoch 1, step 9300 - Total_idx 506 - Train Loss: 0.13976014491170644 - Test Loss: 0.09922894290648401\n",
      "Epoch 1, step 9350 - Total_idx 507 - Train Loss: 0.08254129694774746 - Test Loss: 0.1045792706310749\n",
      "Epoch 1, step 9400 - Total_idx 508 - Train Loss: 0.07124561721459031 - Test Loss: 0.10145233282819391\n",
      "Epoch 1, step 9450 - Total_idx 509 - Train Loss: 0.08201640787534416 - Test Loss: 0.11695154211483896\n",
      "Epoch 1, step 9500 - Total_idx 510 - Train Loss: 0.06760834103450179 - Test Loss: 0.08321058377623558\n",
      "Epoch 1, step 9550 - Total_idx 511 - Train Loss: 0.09300012227147818 - Test Loss: 0.08235857952386141\n",
      "Epoch 1, step 9600 - Total_idx 512 - Train Loss: 0.093915253886953 - Test Loss: 0.07890246724709868\n",
      "Epoch 1, step 9650 - Total_idx 513 - Train Loss: 0.0987344730179757 - Test Loss: 0.0411215000320226\n",
      "Epoch 1, step 9700 - Total_idx 514 - Train Loss: 0.10197954970411956 - Test Loss: 0.09375376026146114\n",
      "Epoch 1, step 9750 - Total_idx 515 - Train Loss: 0.08029281925410033 - Test Loss: 0.08947170581668615\n",
      "Epoch 1, step 9800 - Total_idx 516 - Train Loss: 0.10898489628918469 - Test Loss: 0.07544079115614295\n",
      "Epoch 1, step 9850 - Total_idx 517 - Train Loss: 0.0994499535486102 - Test Loss: 0.08625233839266003\n",
      "Epoch 1, step 9900 - Total_idx 518 - Train Loss: 0.09953101936727762 - Test Loss: 0.0767100581433624\n",
      "Epoch 1, step 9950 - Total_idx 519 - Train Loss: 0.14282009505666793 - Test Loss: 0.10388349471613764\n",
      "Epoch 1, step 10000 - Total_idx 520 - Train Loss: 0.12901548175141214 - Test Loss: 0.17726744553074242\n",
      "Epoch 1, step 10050 - Total_idx 521 - Train Loss: 0.09741058533079922 - Test Loss: 0.13002407029271126\n",
      "Epoch 1, step 10100 - Total_idx 522 - Train Loss: 0.08641169141978025 - Test Loss: 0.07510488675907254\n",
      "Epoch 1, step 10150 - Total_idx 523 - Train Loss: 0.08245045362971723 - Test Loss: 0.11372886151075363\n",
      "Epoch 1, step 10200 - Total_idx 524 - Train Loss: 0.07639356525614857 - Test Loss: 0.07872512438334525\n",
      "Epoch 1, step 10250 - Total_idx 525 - Train Loss: 0.1285038263350725 - Test Loss: 0.06435052622109652\n",
      "Epoch 1, step 10300 - Total_idx 526 - Train Loss: 0.08977022139355541 - Test Loss: 0.22559670465998352\n",
      "Epoch 1, step 10350 - Total_idx 527 - Train Loss: 0.08498052870854736 - Test Loss: 0.08014166252687574\n",
      "Epoch 1, step 10400 - Total_idx 528 - Train Loss: 0.10179907630197704 - Test Loss: 0.08652079841122032\n",
      "Epoch 1, step 10450 - Total_idx 529 - Train Loss: 0.13807103074155747 - Test Loss: 0.08384222527965904\n",
      "Epoch 1, step 10500 - Total_idx 530 - Train Loss: 0.08501487136818468 - Test Loss: 0.08384045436978341\n",
      "Epoch 1, step 10550 - Total_idx 531 - Train Loss: 0.09775820422917604 - Test Loss: 0.15031010499224068\n",
      "Epoch 1, step 10600 - Total_idx 532 - Train Loss: 0.10022297315299511 - Test Loss: 0.06765623586252331\n",
      "Epoch 1, step 10650 - Total_idx 533 - Train Loss: 0.0944845165591687 - Test Loss: 0.18117446638643742\n",
      "Epoch 1, step 10700 - Total_idx 534 - Train Loss: 0.12681933634914458 - Test Loss: 0.05717211118899286\n",
      "Epoch 1, step 10750 - Total_idx 535 - Train Loss: 0.09626378310844302 - Test Loss: 0.13201319868676364\n",
      "Epoch 1, step 10800 - Total_idx 536 - Train Loss: 0.1091697645559907 - Test Loss: 0.14829628355801105\n",
      "Epoch 1, step 10850 - Total_idx 537 - Train Loss: 0.11575158813036979 - Test Loss: 0.11772491931915283\n",
      "Epoch 1, step 10900 - Total_idx 538 - Train Loss: 0.08809900367632509 - Test Loss: 0.1461015220731497\n",
      "Epoch 1, step 10950 - Total_idx 539 - Train Loss: 0.10360254706814885 - Test Loss: 0.07993589602410793\n",
      "Epoch 1, step 11000 - Total_idx 540 - Train Loss: 0.11314729358069599 - Test Loss: 0.09770530760288239\n",
      "Epoch 1, step 11050 - Total_idx 541 - Train Loss: 0.10230756680481136 - Test Loss: 0.09085565339773893\n",
      "Epoch 1, step 11100 - Total_idx 542 - Train Loss: 0.09602390534244477 - Test Loss: 0.07484077727422118\n",
      "Epoch 1, step 11150 - Total_idx 543 - Train Loss: 0.06691991872154177 - Test Loss: 0.10523638278245925\n",
      "Epoch 1, step 11200 - Total_idx 544 - Train Loss: 0.08699997067451477 - Test Loss: 0.14474892290309072\n",
      "Epoch 1, step 11250 - Total_idx 545 - Train Loss: 0.07641820207238198 - Test Loss: 0.057787317084148526\n",
      "Epoch 1, step 11300 - Total_idx 546 - Train Loss: 0.11323233128525317 - Test Loss: 0.13145330119878054\n",
      "Epoch 1, step 11350 - Total_idx 547 - Train Loss: 0.10450522783212364 - Test Loss: 0.041930956905707716\n",
      "Epoch 1, step 11400 - Total_idx 548 - Train Loss: 0.08310965107753873 - Test Loss: 0.1856601672247052\n",
      "Epoch 1, step 11450 - Total_idx 549 - Train Loss: 0.10663982335478067 - Test Loss: 0.07508926535956562\n",
      "Epoch 1, step 11500 - Total_idx 550 - Train Loss: 0.0945383269060403 - Test Loss: 0.08510164003819227\n",
      "Epoch 1, step 11550 - Total_idx 551 - Train Loss: 0.12672359499149025 - Test Loss: 0.10028235623613\n",
      "Epoch 1, step 11600 - Total_idx 552 - Train Loss: 0.09193139478564262 - Test Loss: 0.10790401417762041\n",
      "Epoch 1, step 11650 - Total_idx 553 - Train Loss: 0.12036491972394288 - Test Loss: 0.1366378702223301\n",
      "Epoch 1, step 11700 - Total_idx 554 - Train Loss: 0.1511695657391101 - Test Loss: 0.13912276765331627\n",
      "Epoch 1, step 11750 - Total_idx 555 - Train Loss: 0.13479222744703293 - Test Loss: 0.10666535813361407\n",
      "Epoch 1, step 11800 - Total_idx 556 - Train Loss: 0.12457277907989919 - Test Loss: 0.054216663306579\n",
      "Epoch 1, step 11850 - Total_idx 557 - Train Loss: 0.0822189313545823 - Test Loss: 0.11743432898074388\n",
      "Epoch 1, step 11900 - Total_idx 558 - Train Loss: 0.06443644227460027 - Test Loss: 0.12412484865635634\n",
      "Epoch 1, step 11950 - Total_idx 559 - Train Loss: 0.1109723305888474 - Test Loss: 0.07827669554390013\n",
      "Epoch 1, step 12000 - Total_idx 560 - Train Loss: 0.08363152981735766 - Test Loss: 0.12013602387160063\n",
      "Epoch 1, step 12050 - Total_idx 561 - Train Loss: 0.08366723119281233 - Test Loss: 0.0586480041500181\n",
      "Epoch 1, step 12100 - Total_idx 562 - Train Loss: 0.10097080128267408 - Test Loss: 0.055078933387994765\n",
      "Epoch 1, step 12150 - Total_idx 563 - Train Loss: 0.08796507772058249 - Test Loss: 0.16161680864170194\n",
      "Epoch 1, step 12200 - Total_idx 564 - Train Loss: 0.13870175898075104 - Test Loss: 0.17629613098688424\n",
      "Epoch 1, step 12250 - Total_idx 565 - Train Loss: 0.08482503521256149 - Test Loss: 0.08584641446359456\n",
      "Epoch 1, step 12300 - Total_idx 566 - Train Loss: 0.11832638017833233 - Test Loss: 0.1144098901655525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 12350 - Total_idx 567 - Train Loss: 0.08973807414062321 - Test Loss: 0.10003283927217126\n",
      "Epoch 1, step 12400 - Total_idx 568 - Train Loss: 0.09251844801940023 - Test Loss: 0.08809472187422215\n",
      "Epoch 1, step 12450 - Total_idx 569 - Train Loss: 0.09170931729488074 - Test Loss: 0.10739872306585312\n",
      "Epoch 1, step 12500 - Total_idx 570 - Train Loss: 0.10270065476186574 - Test Loss: 0.0673308378085494\n",
      "Epoch 1, step 12550 - Total_idx 571 - Train Loss: 0.07722829283215106 - Test Loss: 0.13259484814479947\n",
      "Epoch 1, step 12600 - Total_idx 572 - Train Loss: 0.07672876446507872 - Test Loss: 0.1292250830680132\n",
      "Epoch 1, step 12650 - Total_idx 573 - Train Loss: 0.06588872475549579 - Test Loss: 0.1384560047648847\n",
      "Epoch 1, step 12700 - Total_idx 574 - Train Loss: 0.09390635810792446 - Test Loss: 0.16756752952933313\n",
      "Epoch 1, step 12750 - Total_idx 575 - Train Loss: 0.0665477099455893 - Test Loss: 0.10698691024444998\n",
      "Epoch 1, step 12800 - Total_idx 576 - Train Loss: 0.07820184951648117 - Test Loss: 0.11158725693821907\n",
      "Epoch 1, step 12850 - Total_idx 577 - Train Loss: 0.06948717018589377 - Test Loss: 0.05751923201605678\n",
      "Epoch 1, step 12900 - Total_idx 578 - Train Loss: 0.07864320478402079 - Test Loss: 0.06540290392003953\n",
      "Epoch 1, step 12950 - Total_idx 579 - Train Loss: 0.1338253378495574 - Test Loss: 0.03303222684189677\n",
      "Epoch 1, step 13000 - Total_idx 580 - Train Loss: 0.08827977509237826 - Test Loss: 0.13355842102319002\n",
      "Epoch 1, step 13050 - Total_idx 581 - Train Loss: 0.08982632200233638 - Test Loss: 0.09246977032162249\n",
      "Epoch 1, step 13100 - Total_idx 582 - Train Loss: 0.08275193274021149 - Test Loss: 0.06392536722123623\n",
      "Epoch 1, step 13150 - Total_idx 583 - Train Loss: 0.07088292418979109 - Test Loss: 0.0273321277461946\n",
      "Epoch 1, step 13200 - Total_idx 584 - Train Loss: 0.07784329566173255 - Test Loss: 0.13001145576126874\n",
      "Epoch 1, step 13250 - Total_idx 585 - Train Loss: 0.1125246652495116 - Test Loss: 0.07663393542170524\n",
      "Epoch 1, step 13300 - Total_idx 586 - Train Loss: 0.09803598852828145 - Test Loss: 0.07998098749667407\n",
      "Epoch 1, step 13350 - Total_idx 587 - Train Loss: 0.06829056447371841 - Test Loss: 0.13010661881417035\n",
      "Epoch 1, step 13400 - Total_idx 588 - Train Loss: 0.0810936819948256 - Test Loss: 0.08381480467505753\n",
      "Epoch 1, step 13450 - Total_idx 589 - Train Loss: 0.09130097500048578 - Test Loss: 0.07047222196124495\n",
      "Epoch 1, step 13500 - Total_idx 590 - Train Loss: 0.08394846151582896 - Test Loss: 0.15129598081111909\n",
      "Epoch 1, step 13550 - Total_idx 591 - Train Loss: 0.10144948319531977 - Test Loss: 0.0626314534805715\n",
      "Epoch 1, step 13600 - Total_idx 592 - Train Loss: 0.06237720035947859 - Test Loss: 0.0759755838662386\n",
      "Epoch 1, step 13650 - Total_idx 593 - Train Loss: 0.08603845307603479 - Test Loss: 0.10682742111384869\n",
      "Epoch 1, step 13700 - Total_idx 594 - Train Loss: 0.07406006373465061 - Test Loss: 0.09888506722636521\n",
      "Epoch 1, step 13750 - Total_idx 595 - Train Loss: 0.11457362205721439 - Test Loss: 0.10804790472611785\n",
      "Epoch 1, step 13800 - Total_idx 596 - Train Loss: 0.082424022462219 - Test Loss: 0.08021694249473513\n",
      "Epoch 1, step 13850 - Total_idx 597 - Train Loss: 0.10645814783871174 - Test Loss: 0.02899730009958148\n",
      "Epoch 1, step 13900 - Total_idx 598 - Train Loss: 0.1034184316918254 - Test Loss: 0.1491932409349829\n",
      "Epoch 1, step 13950 - Total_idx 599 - Train Loss: 0.12534189636819065 - Test Loss: 0.1470491137355566\n",
      "Epoch 1, step 14000 - Total_idx 600 - Train Loss: 0.07982506386004388 - Test Loss: 0.12409658809192478\n",
      "Epoch 1, step 14050 - Total_idx 601 - Train Loss: 0.092068035826087 - Test Loss: 0.06452108486555516\n",
      "Epoch 1, step 14100 - Total_idx 602 - Train Loss: 0.11970824808813632 - Test Loss: 0.10980802318081259\n",
      "Epoch 1, step 14150 - Total_idx 603 - Train Loss: 0.1347965589351952 - Test Loss: 0.03832934172824025\n",
      "Epoch 1, step 14200 - Total_idx 604 - Train Loss: 0.09391408815979957 - Test Loss: 0.11722444351762533\n",
      "Epoch 1, step 14250 - Total_idx 605 - Train Loss: 0.13715958853252233 - Test Loss: 0.12178448587656021\n",
      "Epoch 1, step 14300 - Total_idx 606 - Train Loss: 0.08524234108626842 - Test Loss: 0.14582474171184004\n",
      "Epoch 1, step 14350 - Total_idx 607 - Train Loss: 0.11891908211633563 - Test Loss: 0.11122358706779778\n",
      "Epoch 1, step 14400 - Total_idx 608 - Train Loss: 0.08882333854213358 - Test Loss: 0.15430175634101034\n",
      "Epoch 1, step 14450 - Total_idx 609 - Train Loss: 0.09947468549013137 - Test Loss: 0.12232852070592344\n",
      "Epoch 1, step 14500 - Total_idx 610 - Train Loss: 0.09740435812622308 - Test Loss: 0.0479830791708082\n",
      "Epoch 1, step 14550 - Total_idx 611 - Train Loss: 0.09720475472509861 - Test Loss: 0.08595743086189031\n",
      "Epoch 1, step 14600 - Total_idx 612 - Train Loss: 0.08127046556212009 - Test Loss: 0.1271968395449221\n",
      "Epoch 1, step 14650 - Total_idx 613 - Train Loss: 0.11211655344814062 - Test Loss: 0.14164188839495181\n",
      "Epoch 1, step 14700 - Total_idx 614 - Train Loss: 0.12545948914252222 - Test Loss: 0.07125691287219524\n",
      "Epoch 1, step 14750 - Total_idx 615 - Train Loss: 0.09692424217239022 - Test Loss: 0.05566602936014533\n",
      "Epoch 1, step 14800 - Total_idx 616 - Train Loss: 0.07769140664488078 - Test Loss: 0.12570375418290497\n",
      "Epoch 1, step 14850 - Total_idx 617 - Train Loss: 0.11111621850170195 - Test Loss: 0.08489641044288873\n",
      "Epoch 1, step 14900 - Total_idx 618 - Train Loss: 0.1205769690964371 - Test Loss: 0.08007594551891088\n",
      "Epoch 1, step 14950 - Total_idx 619 - Train Loss: 0.07731285082176327 - Test Loss: 0.11994455656968057\n",
      "Epoch 1, step 15000 - Total_idx 620 - Train Loss: 0.09000315623357892 - Test Loss: 0.10731390751898288\n",
      "Epoch 1, step 15050 - Total_idx 621 - Train Loss: 0.09764486494474113 - Test Loss: 0.11113466094247997\n",
      "Epoch 1, step 15100 - Total_idx 622 - Train Loss: 0.06268000815995038 - Test Loss: 0.09458264969289303\n",
      "Epoch 1, step 15150 - Total_idx 623 - Train Loss: 0.10087510129436851 - Test Loss: 0.06696337764151394\n",
      "Epoch 1, step 15200 - Total_idx 624 - Train Loss: 0.12342136229388416 - Test Loss: 0.0985599160194397\n",
      "Epoch 1, step 15250 - Total_idx 625 - Train Loss: 0.10514347393997013 - Test Loss: 0.07422078638337552\n",
      "Epoch 1, step 15300 - Total_idx 626 - Train Loss: 0.11168464655056595 - Test Loss: 0.11245376644656062\n",
      "Epoch 1, step 15350 - Total_idx 627 - Train Loss: 0.10517194475978613 - Test Loss: 0.08244665986858309\n",
      "Epoch 1, step 15400 - Total_idx 628 - Train Loss: 0.06324897021986545 - Test Loss: 0.08416856080293655\n",
      "Epoch 1, step 15450 - Total_idx 629 - Train Loss: 0.10259190436452627 - Test Loss: 0.05854642437770963\n",
      "Epoch 1, step 15500 - Total_idx 630 - Train Loss: 0.08444480961188675 - Test Loss: 0.10330417882651091\n",
      "Epoch 1, step 15550 - Total_idx 631 - Train Loss: 0.10360043541528284 - Test Loss: 0.04128739116713405\n",
      "Epoch 1, step 15600 - Total_idx 632 - Train Loss: 0.10183402497321367 - Test Loss: 0.03439178443513811\n",
      "Epoch 1, step 15650 - Total_idx 633 - Train Loss: 0.11788028705865145 - Test Loss: 0.08422852070070803\n",
      "Epoch 1, step 15700 - Total_idx 634 - Train Loss: 0.10190302681177854 - Test Loss: 0.07364498735405504\n",
      "Epoch 1, step 15750 - Total_idx 635 - Train Loss: 0.08297343576326967 - Test Loss: 0.11055975123308599\n",
      "Epoch 1, step 15800 - Total_idx 636 - Train Loss: 0.10817006655037403 - Test Loss: 0.04780284035950899\n",
      "Epoch 1, step 15850 - Total_idx 637 - Train Loss: 0.08397696359083057 - Test Loss: 0.12183634117245674\n",
      "Epoch 1, step 15900 - Total_idx 638 - Train Loss: 0.08147734891623258 - Test Loss: 0.20161617598496379\n",
      "Epoch 1, step 15950 - Total_idx 639 - Train Loss: 0.12455242805182934 - Test Loss: 0.1642339127138257\n",
      "\t* ===================================== *\n",
      "\t* Epoch 1 :\n",
      "\t* \t Average Train Loss: 0.10117884601876721\n",
      "\t* \t Average Test Loss: 0.10076084760934463\n",
      "Epoch 2, step 0 - Total_idx 640 - Train Loss: 0.030909055843949318 - Test Loss: 0.0988880286924541\n",
      "Epoch 2, step 50 - Total_idx 641 - Train Loss: 0.10196916659362615 - Test Loss: 0.036573233595117924\n",
      "Epoch 2, step 100 - Total_idx 642 - Train Loss: 0.1230588642321527 - Test Loss: 0.14430311848409474\n",
      "Epoch 2, step 150 - Total_idx 643 - Train Loss: 0.08976207848638296 - Test Loss: 0.05950591606087983\n",
      "Epoch 2, step 200 - Total_idx 644 - Train Loss: 0.06946284611709416 - Test Loss: 0.08945977007970214\n",
      "Epoch 2, step 250 - Total_idx 645 - Train Loss: 0.07131460743024945 - Test Loss: 0.050032817479223016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, step 300 - Total_idx 646 - Train Loss: 0.07765090917237102 - Test Loss: 0.13522590468637646\n",
      "Epoch 2, step 350 - Total_idx 647 - Train Loss: 0.08475933462381363 - Test Loss: 0.03123183879069984\n",
      "Epoch 2, step 400 - Total_idx 648 - Train Loss: 0.11130572067573667 - Test Loss: 0.10341641497798264\n",
      "Epoch 2, step 450 - Total_idx 649 - Train Loss: 0.08451177972368897 - Test Loss: 0.03977204477414489\n",
      "Epoch 2, step 500 - Total_idx 650 - Train Loss: 0.10310608154162765 - Test Loss: 0.06467214734293521\n",
      "Epoch 2, step 550 - Total_idx 651 - Train Loss: 0.11294363397173583 - Test Loss: 0.15054667936637997\n",
      "Epoch 2, step 600 - Total_idx 652 - Train Loss: 0.06376254634000361 - Test Loss: 0.028503395710140468\n",
      "Epoch 2, step 650 - Total_idx 653 - Train Loss: 0.07358113001100719 - Test Loss: 0.14549576556310057\n",
      "Epoch 2, step 700 - Total_idx 654 - Train Loss: 0.09347308912314474 - Test Loss: 0.1540081687271595\n",
      "Epoch 2, step 750 - Total_idx 655 - Train Loss: 0.08552975855767726 - Test Loss: 0.02436342751607299\n",
      "Epoch 2, step 800 - Total_idx 656 - Train Loss: 0.0839192658290267 - Test Loss: 0.1520434762351215\n",
      "Epoch 2, step 850 - Total_idx 657 - Train Loss: 0.08233702852390706 - Test Loss: 0.1455426275730133\n",
      "Epoch 2, step 900 - Total_idx 658 - Train Loss: 0.08746309233829379 - Test Loss: 0.08851497904397547\n",
      "Epoch 2, step 950 - Total_idx 659 - Train Loss: 0.0989038824569434 - Test Loss: 0.08863088190555572\n",
      "Epoch 2, step 1000 - Total_idx 660 - Train Loss: 0.06582935297861696 - Test Loss: 0.10254372227936984\n",
      "Epoch 2, step 1050 - Total_idx 661 - Train Loss: 0.09896529087796807 - Test Loss: 0.1426889210473746\n",
      "Epoch 2, step 1100 - Total_idx 662 - Train Loss: 0.06725201964844019 - Test Loss: 0.20915162079036237\n",
      "Epoch 2, step 1150 - Total_idx 663 - Train Loss: 0.09130557703785598 - Test Loss: 0.10370203899219632\n",
      "Epoch 2, step 1200 - Total_idx 664 - Train Loss: 0.09277740138582885 - Test Loss: 0.060351126454770566\n",
      "Epoch 2, step 1250 - Total_idx 665 - Train Loss: 0.06321700997650623 - Test Loss: 0.0660515571013093\n",
      "Epoch 2, step 1300 - Total_idx 666 - Train Loss: 0.09734135248698295 - Test Loss: 0.03405251810327172\n",
      "Epoch 2, step 1350 - Total_idx 667 - Train Loss: 0.09811929234303535 - Test Loss: 0.05153691503219306\n",
      "Epoch 2, step 1400 - Total_idx 668 - Train Loss: 0.09756056454032659 - Test Loss: 0.13203381821513177\n",
      "Epoch 2, step 1450 - Total_idx 669 - Train Loss: 0.11065018609166145 - Test Loss: 0.1111046067904681\n",
      "Epoch 2, step 1500 - Total_idx 670 - Train Loss: 0.08592542354017496 - Test Loss: 0.23927027247846128\n",
      "Epoch 2, step 1550 - Total_idx 671 - Train Loss: 0.08997523098252713 - Test Loss: 0.16710601132363082\n",
      "Epoch 2, step 1600 - Total_idx 672 - Train Loss: 0.09998038445599378 - Test Loss: 0.13915478689596056\n",
      "Epoch 2, step 1650 - Total_idx 673 - Train Loss: 0.08838727192953229 - Test Loss: 0.1015456922352314\n",
      "Epoch 2, step 1700 - Total_idx 674 - Train Loss: 0.0648971800878644 - Test Loss: 0.07580787660554052\n",
      "Epoch 2, step 1750 - Total_idx 675 - Train Loss: 0.07447688097134232 - Test Loss: 0.09810968022793531\n",
      "Epoch 2, step 1800 - Total_idx 676 - Train Loss: 0.0980822525266558 - Test Loss: 0.0876741643063724\n",
      "Epoch 2, step 1850 - Total_idx 677 - Train Loss: 0.06213055912405253 - Test Loss: 0.044960935972630976\n",
      "Epoch 2, step 1900 - Total_idx 678 - Train Loss: 0.10283410695381462 - Test Loss: 0.054988170089200136\n",
      "Epoch 2, step 1950 - Total_idx 679 - Train Loss: 0.10729432698339224 - Test Loss: 0.09769833348691463\n",
      "Epoch 2, step 2000 - Total_idx 680 - Train Loss: 0.13532229262404144 - Test Loss: 0.058289230521768334\n",
      "Epoch 2, step 2050 - Total_idx 681 - Train Loss: 0.07263857370242477 - Test Loss: 0.1060162921436131\n",
      "Epoch 2, step 2100 - Total_idx 682 - Train Loss: 0.09380854903720319 - Test Loss: 0.1649296335875988\n",
      "Epoch 2, step 2150 - Total_idx 683 - Train Loss: 0.11072541068308056 - Test Loss: 0.04372049681842327\n",
      "Epoch 2, step 2200 - Total_idx 684 - Train Loss: 0.08109240598045289 - Test Loss: 0.09861780498176813\n",
      "Epoch 2, step 2250 - Total_idx 685 - Train Loss: 0.08060758604668081 - Test Loss: 0.11698776111006737\n",
      "Epoch 2, step 2300 - Total_idx 686 - Train Loss: 0.11031471462920309 - Test Loss: 0.04478386081755161\n",
      "Epoch 2, step 2350 - Total_idx 687 - Train Loss: 0.1309034047089517 - Test Loss: 0.07884729118086398\n",
      "Epoch 2, step 2400 - Total_idx 688 - Train Loss: 0.07102404940873384 - Test Loss: 0.13959657680243254\n",
      "Epoch 2, step 2450 - Total_idx 689 - Train Loss: 0.12775486374273895 - Test Loss: 0.15974970823153853\n",
      "Epoch 2, step 2500 - Total_idx 690 - Train Loss: 0.13533250777982175 - Test Loss: 0.06515299403108657\n",
      "Epoch 2, step 2550 - Total_idx 691 - Train Loss: 0.07021025786176324 - Test Loss: 0.07915206574834883\n",
      "Epoch 2, step 2600 - Total_idx 692 - Train Loss: 0.06903012852184474 - Test Loss: 0.16099920612759888\n",
      "Epoch 2, step 2650 - Total_idx 693 - Train Loss: 0.08415200218558311 - Test Loss: 0.046352163050323726\n",
      "Epoch 2, step 2700 - Total_idx 694 - Train Loss: 0.04897089564241469 - Test Loss: 0.07510744910687209\n",
      "Epoch 2, step 2750 - Total_idx 695 - Train Loss: 0.07634035251103341 - Test Loss: 0.05321622365154326\n",
      "Epoch 2, step 2800 - Total_idx 696 - Train Loss: 0.11513738499488682 - Test Loss: 0.04033708972856402\n",
      "Epoch 2, step 2850 - Total_idx 697 - Train Loss: 0.07726314366795123 - Test Loss: 0.08767998851835727\n",
      "Epoch 2, step 2900 - Total_idx 698 - Train Loss: 0.08179916358552873 - Test Loss: 0.05182923730462789\n",
      "Epoch 2, step 2950 - Total_idx 699 - Train Loss: 0.11029690622352063 - Test Loss: 0.08058587396517396\n",
      "Epoch 2, step 3000 - Total_idx 700 - Train Loss: 0.07099701692350209 - Test Loss: 0.020497786486521362\n",
      "Epoch 2, step 3050 - Total_idx 701 - Train Loss: 0.09456697693094611 - Test Loss: 0.0609231136739254\n",
      "Epoch 2, step 3100 - Total_idx 702 - Train Loss: 0.11757525407709181 - Test Loss: 0.1465159760788083\n",
      "Epoch 2, step 3150 - Total_idx 703 - Train Loss: 0.08748891360126436 - Test Loss: 0.10687764356844127\n",
      "Epoch 2, step 3200 - Total_idx 704 - Train Loss: 0.11801258644089102 - Test Loss: 0.09397315727546811\n",
      "Epoch 2, step 3250 - Total_idx 705 - Train Loss: 0.09139628472737968 - Test Loss: 0.07851338493637741\n",
      "Epoch 2, step 3300 - Total_idx 706 - Train Loss: 0.08210195067338645 - Test Loss: 0.0835120010189712\n",
      "Epoch 2, step 3350 - Total_idx 707 - Train Loss: 0.11013687597587704 - Test Loss: 0.09168731700628996\n",
      "Epoch 2, step 3400 - Total_idx 708 - Train Loss: 0.08695810328237713 - Test Loss: 0.09157529841177166\n",
      "Epoch 2, step 3450 - Total_idx 709 - Train Loss: 0.08885641581378877 - Test Loss: 0.04997822102159262\n",
      "Epoch 2, step 3500 - Total_idx 710 - Train Loss: 0.12393387766554952 - Test Loss: 0.06810141350142658\n",
      "Epoch 2, step 3550 - Total_idx 711 - Train Loss: 0.09788967998698354 - Test Loss: 0.08443497931584716\n",
      "Epoch 2, step 3600 - Total_idx 712 - Train Loss: 0.08028006370179355 - Test Loss: 0.06302602305077017\n",
      "Epoch 2, step 3650 - Total_idx 713 - Train Loss: 0.10384729472920298 - Test Loss: 0.15120217222720383\n",
      "Epoch 2, step 3700 - Total_idx 714 - Train Loss: 0.06880264300853015 - Test Loss: 0.05701906648464501\n",
      "Epoch 2, step 3750 - Total_idx 715 - Train Loss: 0.09435171773657203 - Test Loss: 0.045174721907824275\n",
      "Epoch 2, step 3800 - Total_idx 716 - Train Loss: 0.09609557195566594 - Test Loss: 0.09167802147567272\n",
      "Epoch 2, step 3850 - Total_idx 717 - Train Loss: 0.05318771299906075 - Test Loss: 0.05174004212021828\n",
      "Epoch 2, step 3900 - Total_idx 718 - Train Loss: 0.09550075572915376 - Test Loss: 0.07948107942938805\n",
      "Epoch 2, step 3950 - Total_idx 719 - Train Loss: 0.11209603144787252 - Test Loss: 0.1149191256146878\n",
      "Epoch 2, step 4000 - Total_idx 720 - Train Loss: 0.09487160431221127 - Test Loss: 0.16288392492569984\n",
      "Epoch 2, step 4050 - Total_idx 721 - Train Loss: 0.08985947185195982 - Test Loss: 0.10559931797906756\n",
      "Epoch 2, step 4100 - Total_idx 722 - Train Loss: 0.05837734396569431 - Test Loss: 0.1624033951666206\n",
      "Epoch 2, step 4150 - Total_idx 723 - Train Loss: 0.07356458993628621 - Test Loss: 0.11833523511886597\n",
      "Epoch 2, step 4200 - Total_idx 724 - Train Loss: 0.079743094984442 - Test Loss: 0.1302733170799911\n",
      "Epoch 2, step 4250 - Total_idx 725 - Train Loss: 0.08415940570645035 - Test Loss: 0.06203961577266455\n",
      "Epoch 2, step 4300 - Total_idx 726 - Train Loss: 0.053849821742624045 - Test Loss: 0.07609168104827405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, step 4350 - Total_idx 727 - Train Loss: 0.06829457003623247 - Test Loss: 0.05821473444812\n",
      "Epoch 2, step 4400 - Total_idx 728 - Train Loss: 0.09904315987601876 - Test Loss: 0.04636343820020557\n",
      "Epoch 2, step 4450 - Total_idx 729 - Train Loss: 0.09729577804915607 - Test Loss: 0.1288621085230261\n",
      "Epoch 2, step 4500 - Total_idx 730 - Train Loss: 0.0807034801878035 - Test Loss: 0.09296930162236094\n",
      "Epoch 2, step 4550 - Total_idx 731 - Train Loss: 0.06859475917182863 - Test Loss: 0.0953331452794373\n",
      "Epoch 2, step 4600 - Total_idx 732 - Train Loss: 0.11287785489577801 - Test Loss: 0.06832784640137106\n",
      "Epoch 2, step 4650 - Total_idx 733 - Train Loss: 0.082624082416296 - Test Loss: 0.11975671253167093\n",
      "Epoch 2, step 4700 - Total_idx 734 - Train Loss: 0.04762562330812216 - Test Loss: 0.0638602439314127\n",
      "Epoch 2, step 4750 - Total_idx 735 - Train Loss: 0.10498527260497212 - Test Loss: 0.10922942333854735\n",
      "Epoch 2, step 4800 - Total_idx 736 - Train Loss: 0.08413086324930191 - Test Loss: 0.08433074187487363\n",
      "Epoch 2, step 4850 - Total_idx 737 - Train Loss: 0.08316316640004516 - Test Loss: 0.1335298115387559\n",
      "Epoch 2, step 4900 - Total_idx 738 - Train Loss: 0.08297330318018795 - Test Loss: 0.059901090385392305\n",
      "Epoch 2, step 4950 - Total_idx 739 - Train Loss: 0.0585293732676655 - Test Loss: 0.15436697769910096\n",
      "Epoch 2, step 5000 - Total_idx 740 - Train Loss: 0.09767780835274607 - Test Loss: 0.1312539427075535\n",
      "Epoch 2, step 5050 - Total_idx 741 - Train Loss: 0.08011681189760565 - Test Loss: 0.06700119371525944\n",
      "Epoch 2, step 5100 - Total_idx 742 - Train Loss: 0.06757201937027275 - Test Loss: 0.09305466804653406\n",
      "Epoch 2, step 5150 - Total_idx 743 - Train Loss: 0.0808129512798041 - Test Loss: 0.06433205977082253\n",
      "Epoch 2, step 5200 - Total_idx 744 - Train Loss: 0.07404397059697658 - Test Loss: 0.06592163490131497\n",
      "Epoch 2, step 5250 - Total_idx 745 - Train Loss: 0.05171920949593187 - Test Loss: 0.04527511433698237\n",
      "Epoch 2, step 5300 - Total_idx 746 - Train Loss: 0.14861206060275436 - Test Loss: 0.10649540978483855\n",
      "Epoch 2, step 5350 - Total_idx 747 - Train Loss: 0.09644132967106998 - Test Loss: 0.041283917240798475\n",
      "Epoch 2, step 5400 - Total_idx 748 - Train Loss: 0.07770997382700444 - Test Loss: 0.23395186448469757\n",
      "Epoch 2, step 5450 - Total_idx 749 - Train Loss: 0.08155335974879563 - Test Loss: 0.11034741667099297\n",
      "Epoch 2, step 5500 - Total_idx 750 - Train Loss: 0.10075899635441601 - Test Loss: 0.11556501286104322\n",
      "Epoch 2, step 5550 - Total_idx 751 - Train Loss: 0.0942715632263571 - Test Loss: 0.13976412722840906\n",
      "Epoch 2, step 5600 - Total_idx 752 - Train Loss: 0.06614880035631358 - Test Loss: 0.11143201030790806\n",
      "Epoch 2, step 5650 - Total_idx 753 - Train Loss: 0.09220624489709735 - Test Loss: 0.1355635492131114\n",
      "Epoch 2, step 5700 - Total_idx 754 - Train Loss: 0.12939185235649348 - Test Loss: 0.046385055594146254\n",
      "Epoch 2, step 5750 - Total_idx 755 - Train Loss: 0.10041767543181777 - Test Loss: 0.12511060726828874\n",
      "Epoch 2, step 5800 - Total_idx 756 - Train Loss: 0.091158925332129 - Test Loss: 0.12450336604379117\n",
      "Epoch 2, step 5850 - Total_idx 757 - Train Loss: 0.0780839362181723 - Test Loss: 0.11710031568072736\n",
      "Epoch 2, step 5900 - Total_idx 758 - Train Loss: 0.077486621029675 - Test Loss: 0.08438264140859246\n",
      "Epoch 2, step 5950 - Total_idx 759 - Train Loss: 0.08865640578791499 - Test Loss: 0.095885364478454\n",
      "Epoch 2, step 6000 - Total_idx 760 - Train Loss: 0.07946247675456107 - Test Loss: 0.033839391404762866\n",
      "Epoch 2, step 6050 - Total_idx 761 - Train Loss: 0.06914712274447084 - Test Loss: 0.06584429405629635\n",
      "Epoch 2, step 6100 - Total_idx 762 - Train Loss: 0.08566339467652143 - Test Loss: 0.03975218590348959\n",
      "Epoch 2, step 6150 - Total_idx 763 - Train Loss: 0.06774656458757818 - Test Loss: 0.11091047995723784\n",
      "Epoch 2, step 6200 - Total_idx 764 - Train Loss: 0.09868314510211348 - Test Loss: 0.057826395193114874\n",
      "Epoch 2, step 6250 - Total_idx 765 - Train Loss: 0.09750594641081989 - Test Loss: 0.06501242402009666\n",
      "Epoch 2, step 6300 - Total_idx 766 - Train Loss: 0.06574510787148029 - Test Loss: 0.09415189982391894\n",
      "Epoch 2, step 6350 - Total_idx 767 - Train Loss: 0.11281008845195174 - Test Loss: 0.24970028595998883\n",
      "Epoch 2, step 6400 - Total_idx 768 - Train Loss: 0.06527295151725411 - Test Loss: 0.07393113099969924\n",
      "Epoch 2, step 6450 - Total_idx 769 - Train Loss: 0.06359846879262478 - Test Loss: 0.0462724884506315\n",
      "Epoch 2, step 6500 - Total_idx 770 - Train Loss: 0.06531395784579218 - Test Loss: 0.07362614017911255\n",
      "Epoch 2, step 6550 - Total_idx 771 - Train Loss: 0.05851425725966692 - Test Loss: 0.09420569380745292\n",
      "Epoch 2, step 6600 - Total_idx 772 - Train Loss: 0.13433736846782268 - Test Loss: 0.08338568480685353\n",
      "Epoch 2, step 6650 - Total_idx 773 - Train Loss: 0.08063455885276198 - Test Loss: 0.10259444643743336\n",
      "Epoch 2, step 6700 - Total_idx 774 - Train Loss: 0.08086704727262259 - Test Loss: 0.07149272612296045\n",
      "Epoch 2, step 6750 - Total_idx 775 - Train Loss: 0.09915988032706081 - Test Loss: 0.052620194386690855\n",
      "Epoch 2, step 6800 - Total_idx 776 - Train Loss: 0.06886271020397544 - Test Loss: 0.04570132778026163\n",
      "Epoch 2, step 6850 - Total_idx 777 - Train Loss: 0.11209033123217522 - Test Loss: 0.07170493500307203\n",
      "Epoch 2, step 6900 - Total_idx 778 - Train Loss: 0.09336988524533808 - Test Loss: 0.06370162963867188\n",
      "Epoch 2, step 6950 - Total_idx 779 - Train Loss: 0.07917844058945775 - Test Loss: 0.09328235306311398\n",
      "Epoch 2, step 7000 - Total_idx 780 - Train Loss: 0.10689381352625787 - Test Loss: 0.10740874260663986\n",
      "Epoch 2, step 7050 - Total_idx 781 - Train Loss: 0.07112273311242462 - Test Loss: 0.09162257881835104\n",
      "Epoch 2, step 7100 - Total_idx 782 - Train Loss: 0.11340057469904423 - Test Loss: 0.08509014602750539\n",
      "Epoch 2, step 7150 - Total_idx 783 - Train Loss: 0.08974698121193797 - Test Loss: 0.08439818252809346\n",
      "Epoch 2, step 7200 - Total_idx 784 - Train Loss: 0.11153495142236353 - Test Loss: 0.07926648897118867\n",
      "Epoch 2, step 7250 - Total_idx 785 - Train Loss: 0.09792637867853045 - Test Loss: 0.06676497533917428\n",
      "Epoch 2, step 7300 - Total_idx 786 - Train Loss: 0.09404831350781023 - Test Loss: 0.1834285918623209\n",
      "Epoch 2, step 7350 - Total_idx 787 - Train Loss: 0.06810629285871983 - Test Loss: 0.09431719267740846\n",
      "Epoch 2, step 7400 - Total_idx 788 - Train Loss: 0.08491508134640753 - Test Loss: 0.0552639476954937\n",
      "Epoch 2, step 7450 - Total_idx 789 - Train Loss: 0.09473710535094142 - Test Loss: 0.11819673608988523\n",
      "Epoch 2, step 7500 - Total_idx 790 - Train Loss: 0.0757596305012703 - Test Loss: 0.07946218405850232\n",
      "Epoch 2, step 7550 - Total_idx 791 - Train Loss: 0.0516719173733145 - Test Loss: 0.0349223661236465\n",
      "Epoch 2, step 7600 - Total_idx 792 - Train Loss: 0.10315163658931852 - Test Loss: 0.10750157660804689\n",
      "Epoch 2, step 7650 - Total_idx 793 - Train Loss: 0.08469432306475938 - Test Loss: 0.07998517826199532\n",
      "Epoch 2, step 7700 - Total_idx 794 - Train Loss: 0.08284052301198244 - Test Loss: 0.08267912124283612\n",
      "Epoch 2, step 7750 - Total_idx 795 - Train Loss: 0.06925037326756865 - Test Loss: 0.06901578805409372\n",
      "Epoch 2, step 7800 - Total_idx 796 - Train Loss: 0.11787672650068998 - Test Loss: 0.13997623762115835\n",
      "Epoch 2, step 7850 - Total_idx 797 - Train Loss: 0.08374376208521425 - Test Loss: 0.07894875938072801\n",
      "Epoch 2, step 7900 - Total_idx 798 - Train Loss: 0.07976015080697835 - Test Loss: 0.08008175161667168\n",
      "Epoch 2, step 7950 - Total_idx 799 - Train Loss: 0.09047982472926379 - Test Loss: 0.08971990486606955\n",
      "Epoch 2, step 8000 - Total_idx 800 - Train Loss: 0.10435821208637208 - Test Loss: 0.09920315281488001\n",
      "Epoch 2, step 8050 - Total_idx 801 - Train Loss: 0.05148611538112163 - Test Loss: 0.03662621537223458\n",
      "Epoch 2, step 8100 - Total_idx 802 - Train Loss: 0.08579464013688266 - Test Loss: 0.1591635090764612\n",
      "Epoch 2, step 8150 - Total_idx 803 - Train Loss: 0.1006673634564504 - Test Loss: 0.11339650866575539\n",
      "Epoch 2, step 8200 - Total_idx 804 - Train Loss: 0.07725602709688247 - Test Loss: 0.0783010505605489\n",
      "Epoch 2, step 8250 - Total_idx 805 - Train Loss: 0.08586842755787075 - Test Loss: 0.07593391966074706\n",
      "Epoch 2, step 8300 - Total_idx 806 - Train Loss: 0.0843370053358376 - Test Loss: 0.07145144143141806\n",
      "Epoch 2, step 8350 - Total_idx 807 - Train Loss: 0.07014662983827293 - Test Loss: 0.0496720076771453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, step 8400 - Total_idx 808 - Train Loss: 0.11580410292372108 - Test Loss: 0.07107898057438433\n",
      "Epoch 2, step 8450 - Total_idx 809 - Train Loss: 0.09252864240668714 - Test Loss: 0.027509059850126506\n",
      "Epoch 2, step 8500 - Total_idx 810 - Train Loss: 0.1148877945356071 - Test Loss: 0.1262067398056388\n",
      "Epoch 2, step 8550 - Total_idx 811 - Train Loss: 0.07097295691259205 - Test Loss: 0.07877920595929026\n",
      "Epoch 2, step 8600 - Total_idx 812 - Train Loss: 0.0857191176712513 - Test Loss: 0.07128677675500512\n",
      "Epoch 2, step 8650 - Total_idx 813 - Train Loss: 0.07811154480092228 - Test Loss: 0.03724237503483892\n",
      "Epoch 2, step 8700 - Total_idx 814 - Train Loss: 0.07412433109246194 - Test Loss: 0.10037389271892608\n",
      "Epoch 2, step 8750 - Total_idx 815 - Train Loss: 0.06389180103316904 - Test Loss: 0.09343959870748222\n",
      "Epoch 2, step 8800 - Total_idx 816 - Train Loss: 0.10807039437815547 - Test Loss: 0.0932562586851418\n",
      "Epoch 2, step 8850 - Total_idx 817 - Train Loss: 0.10281945056281984 - Test Loss: 0.10311114494688808\n",
      "Epoch 2, step 8900 - Total_idx 818 - Train Loss: 0.089083382897079 - Test Loss: 0.08562542907893658\n",
      "Epoch 2, step 8950 - Total_idx 819 - Train Loss: 0.07176067957654596 - Test Loss: 0.14052253239788115\n",
      "Epoch 2, step 9000 - Total_idx 820 - Train Loss: 0.09945896068587899 - Test Loss: 0.04122418775223195\n",
      "Epoch 2, step 9050 - Total_idx 821 - Train Loss: 0.09654979118146002 - Test Loss: 0.21410268638283014\n",
      "Epoch 2, step 9100 - Total_idx 822 - Train Loss: 0.09571448917500675 - Test Loss: 0.09296992104500532\n",
      "Epoch 2, step 9150 - Total_idx 823 - Train Loss: 0.05593228512443602 - Test Loss: 0.12172179417684674\n",
      "Epoch 2, step 9200 - Total_idx 824 - Train Loss: 0.09601367444265634 - Test Loss: 0.09994096881709993\n",
      "Epoch 2, step 9250 - Total_idx 825 - Train Loss: 0.11778839179314674 - Test Loss: 0.10474032256752253\n",
      "Epoch 2, step 9300 - Total_idx 826 - Train Loss: 0.08048510632477701 - Test Loss: 0.05575047940947116\n",
      "Epoch 2, step 9350 - Total_idx 827 - Train Loss: 0.09060018476098776 - Test Loss: 0.16298695686273276\n",
      "Epoch 2, step 9400 - Total_idx 828 - Train Loss: 0.07283321570605039 - Test Loss: 0.054049644898623225\n",
      "Epoch 2, step 9450 - Total_idx 829 - Train Loss: 0.10548698110505939 - Test Loss: 0.07169994493015111\n",
      "Epoch 2, step 9500 - Total_idx 830 - Train Loss: 0.10259604051709176 - Test Loss: 0.027800587262026964\n",
      "Epoch 2, step 9550 - Total_idx 831 - Train Loss: 0.09721697704866529 - Test Loss: 0.14929484266322107\n",
      "Epoch 2, step 9600 - Total_idx 832 - Train Loss: 0.09995700293686241 - Test Loss: 0.08411963982507586\n",
      "Epoch 2, step 9650 - Total_idx 833 - Train Loss: 0.07448820572346448 - Test Loss: 0.07890602042898535\n",
      "Epoch 2, step 9700 - Total_idx 834 - Train Loss: 0.07778977601788938 - Test Loss: 0.13964611678384245\n",
      "Epoch 2, step 9750 - Total_idx 835 - Train Loss: 0.09100010325200855 - Test Loss: 0.05319424336776137\n",
      "Epoch 2, step 9800 - Total_idx 836 - Train Loss: 0.07419577741064132 - Test Loss: 0.04835559250786901\n",
      "Epoch 2, step 9850 - Total_idx 837 - Train Loss: 0.10552966544404625 - Test Loss: 0.07374580074101686\n",
      "Epoch 2, step 9900 - Total_idx 838 - Train Loss: 0.07581681903917342 - Test Loss: 0.06711327503435313\n",
      "Epoch 2, step 9950 - Total_idx 839 - Train Loss: 0.1097097287978977 - Test Loss: 0.07558082707691938\n",
      "Epoch 2, step 10000 - Total_idx 840 - Train Loss: 0.07749273834750056 - Test Loss: 0.032022258685901764\n",
      "Epoch 2, step 10050 - Total_idx 841 - Train Loss: 0.08586618000641465 - Test Loss: 0.1414229420479387\n",
      "Epoch 2, step 10100 - Total_idx 842 - Train Loss: 0.08939769242890179 - Test Loss: 0.0559343152679503\n",
      "Epoch 2, step 10150 - Total_idx 843 - Train Loss: 0.057674111891537906 - Test Loss: 0.09813405750319362\n",
      "Epoch 2, step 10200 - Total_idx 844 - Train Loss: 0.10882145600859076 - Test Loss: 0.04919031853787601\n",
      "Epoch 2, step 10250 - Total_idx 845 - Train Loss: 0.05318204799667001 - Test Loss: 0.09119924819096922\n",
      "Epoch 2, step 10300 - Total_idx 846 - Train Loss: 0.12878493488766252 - Test Loss: 0.07300514332018793\n",
      "Epoch 2, step 10350 - Total_idx 847 - Train Loss: 0.0663219982990995 - Test Loss: 0.1359396579209715\n",
      "Epoch 2, step 10400 - Total_idx 848 - Train Loss: 0.08732649111188948 - Test Loss: 0.08946519396267831\n",
      "Epoch 2, step 10450 - Total_idx 849 - Train Loss: 0.05177074734121561 - Test Loss: 0.16251712010707706\n",
      "Epoch 2, step 10500 - Total_idx 850 - Train Loss: 0.07264830264262855 - Test Loss: 0.09816089519299567\n",
      "Epoch 2, step 10550 - Total_idx 851 - Train Loss: 0.09450875381007791 - Test Loss: 0.14016106817871332\n",
      "Epoch 2, step 10600 - Total_idx 852 - Train Loss: 0.07216716095339507 - Test Loss: 0.10425439048558474\n",
      "Epoch 2, step 10650 - Total_idx 853 - Train Loss: 0.10626084074378013 - Test Loss: 0.08323665275238454\n",
      "Epoch 2, step 10700 - Total_idx 854 - Train Loss: 0.07684374237433075 - Test Loss: 0.058830793807283045\n",
      "Epoch 2, step 10750 - Total_idx 855 - Train Loss: 0.09204519617836922 - Test Loss: 0.07433421292807907\n",
      "Epoch 2, step 10800 - Total_idx 856 - Train Loss: 0.08864002821035682 - Test Loss: 0.13061484107747673\n",
      "Epoch 2, step 10850 - Total_idx 857 - Train Loss: 0.10207759560085833 - Test Loss: 0.0431764748878777\n",
      "Epoch 2, step 10900 - Total_idx 858 - Train Loss: 0.06775900464504957 - Test Loss: 0.04188835071399808\n",
      "Epoch 2, step 10950 - Total_idx 859 - Train Loss: 0.07838317309506238 - Test Loss: 0.11871859002858401\n",
      "Epoch 2, step 11000 - Total_idx 860 - Train Loss: 0.10742570455186069 - Test Loss: 0.18265754459425806\n",
      "Epoch 2, step 11050 - Total_idx 861 - Train Loss: 0.057129533742554486 - Test Loss: 0.05239653321914375\n",
      "Epoch 2, step 11100 - Total_idx 862 - Train Loss: 0.06900017400737851 - Test Loss: 0.15205429983325303\n",
      "Epoch 2, step 11150 - Total_idx 863 - Train Loss: 0.08803160869516433 - Test Loss: 0.13290043505840005\n",
      "Epoch 2, step 11200 - Total_idx 864 - Train Loss: 0.07180151459295303 - Test Loss: 0.08848029742948711\n",
      "Epoch 2, step 11250 - Total_idx 865 - Train Loss: 0.0723271710332483 - Test Loss: 0.04072268535383046\n",
      "Epoch 2, step 11300 - Total_idx 866 - Train Loss: 0.1118451178073883 - Test Loss: 0.0808636233676225\n",
      "Epoch 2, step 11350 - Total_idx 867 - Train Loss: 0.07694263589568436 - Test Loss: 0.049015944031998514\n",
      "Epoch 2, step 11400 - Total_idx 868 - Train Loss: 0.061776296328753234 - Test Loss: 0.12392918155528605\n",
      "Epoch 2, step 11450 - Total_idx 869 - Train Loss: 0.0596158140944317 - Test Loss: 0.10014770547859371\n",
      "Epoch 2, step 11500 - Total_idx 870 - Train Loss: 0.10758506339974701 - Test Loss: 0.16273997891694308\n",
      "Epoch 2, step 11550 - Total_idx 871 - Train Loss: 0.07288859786000103 - Test Loss: 0.07494370893109589\n",
      "Epoch 2, step 11600 - Total_idx 872 - Train Loss: 0.10856286863330751 - Test Loss: 0.10855458192527294\n",
      "Epoch 2, step 11650 - Total_idx 873 - Train Loss: 0.09359067821875215 - Test Loss: 0.058374303160235286\n",
      "Epoch 2, step 11700 - Total_idx 874 - Train Loss: 0.0932961856899783 - Test Loss: 0.07794458102434873\n",
      "Epoch 2, step 11750 - Total_idx 875 - Train Loss: 0.07804399025626481 - Test Loss: 0.06972037828527392\n",
      "Epoch 2, step 11800 - Total_idx 876 - Train Loss: 0.07595626363996416 - Test Loss: 0.11237421086989344\n",
      "Epoch 2, step 11850 - Total_idx 877 - Train Loss: 0.08435147372074425 - Test Loss: 0.08413489661179482\n",
      "Epoch 2, step 11900 - Total_idx 878 - Train Loss: 0.06905104780569672 - Test Loss: 0.11782671851105989\n",
      "Epoch 2, step 11950 - Total_idx 879 - Train Loss: 0.08141797104850411 - Test Loss: 0.058052991330623624\n",
      "Epoch 2, step 12000 - Total_idx 880 - Train Loss: 0.06974452761933207 - Test Loss: 0.07321321689523756\n",
      "Epoch 2, step 12050 - Total_idx 881 - Train Loss: 0.09037667864002287 - Test Loss: 0.13948488724417984\n",
      "Epoch 2, step 12100 - Total_idx 882 - Train Loss: 0.08742268469184637 - Test Loss: 0.052802531188353895\n",
      "Epoch 2, step 12150 - Total_idx 883 - Train Loss: 0.11124553977511824 - Test Loss: 0.05602519942913205\n",
      "Epoch 2, step 12200 - Total_idx 884 - Train Loss: 0.06633510601706803 - Test Loss: 0.07993839145638049\n",
      "Epoch 2, step 12250 - Total_idx 885 - Train Loss: 0.07527134782634676 - Test Loss: 0.08534140912815928\n",
      "Epoch 2, step 12300 - Total_idx 886 - Train Loss: 0.09533638609573244 - Test Loss: 0.1263010048540309\n",
      "Epoch 2, step 12350 - Total_idx 887 - Train Loss: 0.09993128567468375 - Test Loss: 0.11482923878356814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, step 12400 - Total_idx 888 - Train Loss: 0.0839137731352821 - Test Loss: 0.1421322325244546\n",
      "Epoch 2, step 12450 - Total_idx 889 - Train Loss: 0.0874749955907464 - Test Loss: 0.027699322486296297\n",
      "Epoch 2, step 12500 - Total_idx 890 - Train Loss: 0.10226347767747938 - Test Loss: 0.1396584883797914\n",
      "Epoch 2, step 12550 - Total_idx 891 - Train Loss: 0.090368158435449 - Test Loss: 0.08455810532905161\n",
      "Epoch 2, step 12600 - Total_idx 892 - Train Loss: 0.0870085918251425 - Test Loss: 0.20271461480297148\n",
      "Epoch 2, step 12650 - Total_idx 893 - Train Loss: 0.07766789481975138 - Test Loss: 0.12742971433326603\n",
      "Epoch 2, step 12700 - Total_idx 894 - Train Loss: 0.1268206206895411 - Test Loss: 0.1420035483315587\n",
      "Epoch 2, step 12750 - Total_idx 895 - Train Loss: 0.10371467250399291 - Test Loss: 0.058191655785776675\n",
      "Epoch 2, step 12800 - Total_idx 896 - Train Loss: 0.09056068645790219 - Test Loss: 0.13296545380726457\n",
      "Epoch 2, step 12850 - Total_idx 897 - Train Loss: 0.09531292365398258 - Test Loss: 0.07088022385723888\n",
      "Epoch 2, step 12900 - Total_idx 898 - Train Loss: 0.08551904253661632 - Test Loss: 0.08240730073302985\n",
      "Epoch 2, step 12950 - Total_idx 899 - Train Loss: 0.09458686166442931 - Test Loss: 0.04923572987318039\n",
      "Epoch 2, step 13000 - Total_idx 900 - Train Loss: 0.06431549587287008 - Test Loss: 0.08650549012236297\n",
      "Epoch 2, step 13050 - Total_idx 901 - Train Loss: 0.08725827581714839 - Test Loss: 0.14203496896661819\n",
      "Epoch 2, step 13100 - Total_idx 902 - Train Loss: 0.0862136937584728 - Test Loss: 0.04109919872134924\n",
      "Epoch 2, step 13150 - Total_idx 903 - Train Loss: 0.06992561005987227 - Test Loss: 0.051694683451205495\n",
      "Epoch 2, step 13200 - Total_idx 904 - Train Loss: 0.1129752976167947 - Test Loss: 0.09470712677575648\n",
      "Epoch 2, step 13250 - Total_idx 905 - Train Loss: 0.09026477708015591 - Test Loss: 0.06709676417522133\n",
      "Epoch 2, step 13300 - Total_idx 906 - Train Loss: 0.09891860742587597 - Test Loss: 0.09685902227647603\n",
      "Epoch 2, step 13350 - Total_idx 907 - Train Loss: 0.07486413918435574 - Test Loss: 0.1043556896969676\n",
      "Epoch 2, step 13400 - Total_idx 908 - Train Loss: 0.09579087501391768 - Test Loss: 0.09938624929636716\n",
      "Epoch 2, step 13450 - Total_idx 909 - Train Loss: 0.098192896572873 - Test Loss: 0.08771246676333248\n",
      "Epoch 2, step 13500 - Total_idx 910 - Train Loss: 0.09281823239289225 - Test Loss: 0.0875292549841106\n",
      "Epoch 2, step 13550 - Total_idx 911 - Train Loss: 0.08161178145557642 - Test Loss: 0.07323989598080516\n",
      "Epoch 2, step 13600 - Total_idx 912 - Train Loss: 0.0742779172770679 - Test Loss: 0.06601832672022283\n",
      "Epoch 2, step 13650 - Total_idx 913 - Train Loss: 0.07856715017929673 - Test Loss: 0.039900214644148946\n",
      "Epoch 2, step 13700 - Total_idx 914 - Train Loss: 0.08527492456138135 - Test Loss: 0.08249967945739627\n",
      "Epoch 2, step 13750 - Total_idx 915 - Train Loss: 0.07990763317793607 - Test Loss: 0.07526825945824385\n",
      "Epoch 2, step 13800 - Total_idx 916 - Train Loss: 0.097392298579216 - Test Loss: 0.06980607071891427\n",
      "Epoch 2, step 13850 - Total_idx 917 - Train Loss: 0.09122637514956296 - Test Loss: 0.08789284867234529\n",
      "Epoch 2, step 13900 - Total_idx 918 - Train Loss: 0.09091945494525135 - Test Loss: 0.06044010915793478\n",
      "Epoch 2, step 13950 - Total_idx 919 - Train Loss: 0.09958153223618865 - Test Loss: 0.083630634797737\n",
      "Epoch 2, step 14000 - Total_idx 920 - Train Loss: 0.06737292792648077 - Test Loss: 0.15970787554979324\n",
      "Epoch 2, step 14050 - Total_idx 921 - Train Loss: 0.1089885272178799 - Test Loss: 0.11196914622560143\n",
      "Epoch 2, step 14100 - Total_idx 922 - Train Loss: 0.0940433109458536 - Test Loss: 0.06538577042520047\n",
      "Epoch 2, step 14150 - Total_idx 923 - Train Loss: 0.08289809984155 - Test Loss: 0.09586411835625767\n",
      "Epoch 2, step 14200 - Total_idx 924 - Train Loss: 0.08112665847875178 - Test Loss: 0.06749515794217587\n",
      "Epoch 2, step 14250 - Total_idx 925 - Train Loss: 0.0755294870538637 - Test Loss: 0.07027846984565259\n",
      "Epoch 2, step 14300 - Total_idx 926 - Train Loss: 0.0859164132270962 - Test Loss: 0.22169417571276426\n",
      "Epoch 2, step 14350 - Total_idx 927 - Train Loss: 0.07647430996410548 - Test Loss: 0.06173467868939042\n",
      "Epoch 2, step 14400 - Total_idx 928 - Train Loss: 0.10714541084133089 - Test Loss: 0.08775284164585173\n",
      "Epoch 2, step 14450 - Total_idx 929 - Train Loss: 0.09922183070331812 - Test Loss: 0.08377104038372636\n",
      "Epoch 2, step 14500 - Total_idx 930 - Train Loss: 0.07789773299358785 - Test Loss: 0.09277901072055102\n",
      "Epoch 2, step 14550 - Total_idx 931 - Train Loss: 0.07585488069802522 - Test Loss: 0.14583614114671944\n",
      "Epoch 2, step 14600 - Total_idx 932 - Train Loss: 0.06847192870918661 - Test Loss: 0.05707382960245013\n",
      "Epoch 2, step 14650 - Total_idx 933 - Train Loss: 0.08137537255883216 - Test Loss: 0.16235464168712496\n",
      "Epoch 2, step 14700 - Total_idx 934 - Train Loss: 0.07146221557632089 - Test Loss: 0.04255292401649058\n",
      "Epoch 2, step 14750 - Total_idx 935 - Train Loss: 0.0931338840490207 - Test Loss: 0.12956620980985462\n",
      "Epoch 2, step 14800 - Total_idx 936 - Train Loss: 0.10044904111884534 - Test Loss: 0.14646706534549594\n",
      "Epoch 2, step 14850 - Total_idx 937 - Train Loss: 0.12160875695291906 - Test Loss: 0.10284741907380521\n",
      "Epoch 2, step 14900 - Total_idx 938 - Train Loss: 0.10404566946439445 - Test Loss: 0.1448572978377342\n",
      "Epoch 2, step 14950 - Total_idx 939 - Train Loss: 0.0889303752221167 - Test Loss: 0.06603592559695244\n",
      "Epoch 2, step 15000 - Total_idx 940 - Train Loss: 0.09436574335210025 - Test Loss: 0.10421523065306246\n",
      "Epoch 2, step 15050 - Total_idx 941 - Train Loss: 0.10180785679724068 - Test Loss: 0.09653473887592554\n",
      "Epoch 2, step 15100 - Total_idx 942 - Train Loss: 0.09966081561520695 - Test Loss: 0.08237195070832967\n",
      "Epoch 2, step 15150 - Total_idx 943 - Train Loss: 0.08991015518084168 - Test Loss: 0.10134163717739283\n",
      "Epoch 2, step 15200 - Total_idx 944 - Train Loss: 0.05329101281240582 - Test Loss: 0.13322081174701453\n",
      "Epoch 2, step 15250 - Total_idx 945 - Train Loss: 0.0932569802319631 - Test Loss: 0.049386762036010626\n",
      "Epoch 2, step 15300 - Total_idx 946 - Train Loss: 0.06968479015398771 - Test Loss: 0.13696638084948062\n",
      "Epoch 2, step 15350 - Total_idx 947 - Train Loss: 0.09644718083553017 - Test Loss: 0.033734001917764544\n",
      "Epoch 2, step 15400 - Total_idx 948 - Train Loss: 0.06777464441955089 - Test Loss: 0.19482426904141903\n",
      "Epoch 2, step 15450 - Total_idx 949 - Train Loss: 0.05543245479464531 - Test Loss: 0.06220728261396289\n",
      "Epoch 2, step 15500 - Total_idx 950 - Train Loss: 0.06976195128634571 - Test Loss: 0.0830616034567356\n",
      "Epoch 2, step 15550 - Total_idx 951 - Train Loss: 0.06594910891726613 - Test Loss: 0.10275767892599105\n",
      "Epoch 2, step 15600 - Total_idx 952 - Train Loss: 0.08106567347887904 - Test Loss: 0.11062801843509078\n",
      "Epoch 2, step 15650 - Total_idx 953 - Train Loss: 0.09211983109358698 - Test Loss: 0.1294215464964509\n",
      "Epoch 2, step 15700 - Total_idx 954 - Train Loss: 0.08823845472186803 - Test Loss: 0.15074622731190174\n",
      "Epoch 2, step 15750 - Total_idx 955 - Train Loss: 0.0735638731205836 - Test Loss: 0.10900776789058\n",
      "Epoch 2, step 15800 - Total_idx 956 - Train Loss: 0.09776819659397006 - Test Loss: 0.05434487541206181\n",
      "Epoch 2, step 15850 - Total_idx 957 - Train Loss: 0.1093569535529241 - Test Loss: 0.10605033542960882\n",
      "Epoch 2, step 15900 - Total_idx 958 - Train Loss: 0.09475591898895801 - Test Loss: 0.11515591493807734\n",
      "Epoch 2, step 15950 - Total_idx 959 - Train Loss: 0.11550889853388072 - Test Loss: 0.0791999900713563\n",
      "\t* ===================================== *\n",
      "\t* Epoch 2 :\n",
      "\t* \t Average Train Loss: 0.08754780685431615\n",
      "\t* \t Average Test Loss: 0.09323058610680164\n",
      "Epoch 3, step 0 - Total_idx 960 - Train Loss: 0.22703227400779724 - Test Loss: 0.11494634593836964\n",
      "Epoch 3, step 50 - Total_idx 961 - Train Loss: 0.09013478201813996 - Test Loss: 0.04908359856344759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-671fe4a35969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train an epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Unif\\BigData Project\\covid-mood-analysis\\CamemBERT\\CamemBERT.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, dataset, nb_epoch)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Backward and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;31m# Store temp loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train an epoch\n",
    "model.train(dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-maryland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
