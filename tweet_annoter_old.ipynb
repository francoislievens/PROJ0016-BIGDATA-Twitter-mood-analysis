{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "little-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "first_ex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turkish-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data/tweetDownloadBE.csv\n",
    "if first_ex:\n",
    "    # Ouverture du dataset complet\n",
    "    df = pd.read_csv('Tweets_data/tweetDownloadBE.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supposed-forwarding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>date\\ttext\\tretweet\\tlike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-12-25</th>\n",
       "      <th>@Angele070 Goeiemorgen Angèle ☕</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Je suis à la page 109 de L'Eschylliade - Aux apparences ne te fieras par Pierre-François Kettler https://t.co/rfg0SFxvl4 via @simplementpro</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>« Impact - New Stages » est lauréat de l’appel à Projet @St_art_invest « Rayonnement Wallonie » - de quoi poursuivre et approfondir toutes les démarches entreprises au Théâtre de Liège en matière d’innovation 🤩🙏</th>\n",
       "      <th>1.0</th>\n",
       "      <th>4.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@DocWissam Faudra quand meme analyser les chiffres et comment nous les avons recolte</th>\n",
       "      <th>0.0</th>\n",
       "      <th>3.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@1908Winko Je ne sais pas, je suis partagé, est ce que sport pro veut dire rendre imaginable ce qui se passe dans le monde du travail (des congés à des moments parfois inhabituels au nom du bien être du travailleur) ? Ok, ils gagnent très bien leur vie mais...</th>\n",
       "      <th>0.0</th>\n",
       "      <th>4.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-12-31</th>\n",
       "      <th>@SophieID5 Une très bonne année à vous et tous vos proches</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Il attendait sagement le retour de papa. Thank God pour cette année qui se termine en beauté. I’m blessed 🙏🏾</th>\n",
       "      <th>1.0</th>\n",
       "      <th>49.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suite d’un très beau repas 🥰😉 à Les plus belles cuvées https://t.co/Thw8msa7JZ</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@TinaSalama2 Merci beaucoup et meilleurs vœux 🙏</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@Romynet J'avais dit \"Franco-belge\", pour représenter justement la différence entre actrice américaine et libraire anglais. 😂 (Pure coïncidence, je viens de voir passer un tweet de High Grant). 😂</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18131115 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        date\\ttext\\tretweet\\tlike\n",
       "2020-12-25 @Angele070 Goeiemorgen Angèle ☕                    0.0 1.0                         NaN\n",
       "           Je suis à la page 109 de L'Eschylliade - Aux ap... 0.0 2.0                         NaN\n",
       "           « Impact - New Stages » est lauréat de l’appel ... 1.0 4.0                         NaN\n",
       "           @DocWissam Faudra quand meme analyser les chiff... 0.0 3.0                         NaN\n",
       "           @1908Winko Je ne sais pas, je suis partagé, est... 0.0 4.0                         NaN\n",
       "...                                                                                           ...\n",
       "2020-12-31 @SophieID5 Une très bonne année à vous et tous ... 0.0 0.0                         NaN\n",
       "           Il attendait sagement le retour de papa. Thank ... 1.0 49.0                        NaN\n",
       "           Suite d’un très beau repas 🥰😉 à Les plus belles... 0.0 2.0                         NaN\n",
       "           @TinaSalama2 Merci beaucoup et meilleurs vœux 🙏    0.0 1.0                         NaN\n",
       "           @Romynet J'avais dit \"Franco-belge\", pour repré... 0.0 0.0                         NaN\n",
       "\n",
       "[18131115 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-guard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_ex:\n",
    "    # We have to split the dataset in subset of 100 000 tweets\n",
    "    reader = open('data/tweetDownloadBE.csv', 'r', encoding='Latin-1')\n",
    "    # Read all lines\n",
    "    lines = reader.readlines()\n",
    "\n",
    "    sub_file_size = 100000\n",
    "    file_idx = 0\n",
    "    total_idx = 0\n",
    "    first = True\n",
    "    # Open the new file to write\n",
    "    file = open('data/sub_tweet_{}.csv'.format(file_idx), 'a', encoding='Latin-1')\n",
    "    # Add headers\n",
    "    headers = ['idx', 'date', 'text', 'retweets', 'likes', '\\n']\n",
    "    headers = '\\t'.join(headers)\n",
    "    file.write(headers)\n",
    "    # Store skipped lines\n",
    "    skipped = 0\n",
    "    for line in tqdm(lines):\n",
    "        if total_idx % sub_file_size == 0 and total_idx != 0:\n",
    "            # Write in a new file\n",
    "            file.close()\n",
    "            file_idx += 1\n",
    "            file = open('data/sub_tweet_{}.csv'.format(file_idx), 'a', encoding='Latin-1')\n",
    "            file.write(headers)\n",
    "        # Avoid headers\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        line = line.split('\\t')\n",
    "        if len(line) < 4:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        try:\n",
    "            writer = [str(total_idx),\n",
    "                      str(line[0]),\n",
    "                      str(line[1]),\n",
    "                      str(line[2]),\n",
    "                      str(line[3]),\n",
    "                      '\\n']\n",
    "            file.write('\\t'.join(writer))\n",
    "            total_idx += 1\n",
    "        except:\n",
    "            skipped += 1\n",
    "    file.close()\n",
    "    reader.close()\n",
    "\n",
    "    print('Skipped lines: {}'.format(skipped))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "virgin-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lsit of files\n",
    "import os\n",
    "general_lst = os.listdir('data/')\n",
    "file_lst = []\n",
    "for itm in general_lst:\n",
    "    if 'sub_tweet' in itm:\n",
    "        file_lst.append(itm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pediatric-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, device='cpu', name='model_B', new=False):\n",
    "\n",
    "        super(BERT_model, self).__init__()\n",
    "\n",
    "        # If we want to overwrite an existing model\n",
    "        if new and os.path.exists('Model/weights_{}.pt'.format(name)):\n",
    "            print('Weights for the model {} already exist: deleting...'.format(name))\n",
    "            os.remove('Model/weights_{}.pt'.format(name))\n",
    "        if new and os.path.exists('Model/train_track_{}.csv'.format(name)):\n",
    "            print('Traininck track for model {} already exist: deleting...'.format(name))\n",
    "            os.remove('Model/train_track_{}.csv'.format(name))\n",
    "\n",
    "        # The name of the model to sore data\n",
    "        self.name = name\n",
    "\n",
    "        # Import Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "        # Import the camembert pre-trained model\n",
    "        self.model = AutoModel.from_pretrained('camembert-base')\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        # If there is already a saved model, load it\n",
    "        if os.path.exists('Model/weights_{}.pt'.format(name)):\n",
    "            print('Existing model loading...')\n",
    "            self.model.load_state_dict(torch.load('Model/weights_{}.pt'.format(name)))\n",
    "\n",
    "        # Load training hystory\n",
    "        self.train_history = None\n",
    "        self.epoch_idx = 0\n",
    "        self.total_idx = 0\n",
    "        if os.path.exists('Model/train_track_{}.csv'.format(name)):\n",
    "            self.train_history = pd.read_csv('Model/train_track_{}.csv'.format(name), sep=';', index_col=None)\n",
    "            if self.train_history.shape[0] >= 1:\n",
    "                self.epoch_idx = np.max(self.train_history['epoch'].to_numpy()) + 1\n",
    "                self.total_idx = np.max(self.train_history['idx'].to_numpy()) + 1\n",
    "        else:\n",
    "            # Write file header\n",
    "            file = open('Model/train_track_{}.csv'.format(name), 'a')\n",
    "            file.write('idx;epoch;batch_idx;train_loss;test_loss;\\n')\n",
    "            file.close()\n",
    "\n",
    "        # Set Evaluation mode\n",
    "        #self.model.eval()\n",
    "\n",
    "\n",
    "        # Dataset hyperparameters\n",
    "        self.train_split = 0.8\n",
    "        self.batch_size = 5\n",
    "\n",
    "        # Dataset for training\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.class_names = None\n",
    "        self.train_size = None\n",
    "\n",
    "        # Dataloaders\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = None\n",
    "        # Loss function\n",
    "        self.loss_fn = None\n",
    "        self.loss_fn_test = None\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "    def predictor(self, sentences):\n",
    "\n",
    "        # Get the number of predictions to do\n",
    "        nb_preds = len(sentences)\n",
    "        # Set eval mode\n",
    "        self.model.eval()\n",
    "        # Warning for memory\n",
    "        batch_size = 10\n",
    "        # Tokenize input sentences\n",
    "        # Function to encode data batch\n",
    "        MAX_LENGTH = 280\n",
    "        encoded_batch = self.tokenizer.batch_encode_plus(\n",
    "            sentences,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Build the dataset\n",
    "        dataset = TensorDataset(\n",
    "            encoded_batch['input_ids'],\n",
    "            encoded_batch['attention_mask']\n",
    "        )\n",
    "        # The data loader\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        # Store predictions of each batch\n",
    "        total_preds = []\n",
    "        # For all batchs\n",
    "        for step, batch, in enumerate(loader):\n",
    "            # Get data\n",
    "            input_id = batch[0].to(self.device)\n",
    "            attention_mask = batch[1].to(self.device)\n",
    "            # Make predictions\n",
    "            #preds = self.model(input_id,\n",
    "            #                   token_type_ids=None,\n",
    "            #                   attention_mask=attention_mask,\n",
    "            #                   labels=None)\n",
    "            preds = self.forward(input_id, attention_mask)\n",
    "            total_preds.append(preds.cpu().detach().numpy())\n",
    "\n",
    "        # Reshape outputs\n",
    "        outputs = np.zeros((nb_preds, 2))\n",
    "\n",
    "        idx = 0\n",
    "        for i in range(0, len(total_preds)):\n",
    "            for j in range(0, len(total_preds[i])):\n",
    "                outputs[idx, :] = total_preds[i][j]\n",
    "                idx += 1\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, input_id, attention_mask):\n",
    "\n",
    "        out = self.model(input_id.to(self.device), attention_mask.to(self.device))\n",
    "\n",
    "        return self.fc(out[1])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "successful-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_model(device='cuda:0', name='model_B', new=False).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "second-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/{}'.format(file_lst[0]), sep='\\t', index_col=None)\n",
    "df = df.rename({'Unnamed: 5': 'class'}, axis='columns')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ancient-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46944726 0.53055269]\n",
      " [0.4694984  0.5305016 ]\n",
      " [0.46934074 0.53065932]\n",
      " [0.46942431 0.53057569]\n",
      " [0.46924147 0.53075856]\n",
      " [0.46927774 0.53072226]\n",
      " [0.46948901 0.53051102]\n",
      " [0.46934652 0.53065348]\n",
      " [0.46938014 0.5306198 ]\n",
      " [0.46924034 0.53075963]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\.conda\\envs\\torch3\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "txt = df['text'].tolist()\n",
    "txt = txt[0:10]\n",
    "prds = model.predictor(txt)\n",
    "print(prds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spatial-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_model(\n",
       "  (model): CamembertModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=500, out_features=250, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=250, out_features=50, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (9): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-stanford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
